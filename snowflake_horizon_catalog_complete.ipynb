{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Snowflake Horizon Catalog - Complete Feature Guide\n",
        "\n",
        "This comprehensive notebook covers all features of Snowflake's Horizon Catalog, including:\n",
        "\n",
        "- **Iceberg Tables** - Open table format support\n",
        "- **Traditional Tables & Views** - Standard Snowflake objects\n",
        "- **AI & ML Objects** - Machine learning models and functions\n",
        "- **Costs & Usage** - Monitoring and optimization\n",
        "- **Data Warehouses** - Compute resource management\n",
        "- **Data Shares** - Secure data sharing\n",
        "- **Semantic Views** - Business-friendly data access\n",
        "- **Databases & Schemas** - Data organization\n",
        "- **Business Glossary** - Data governance and documentation\n",
        "- **Workflows** - Automated data pipelines\n",
        "- **RBAC** - Role-based access control\n",
        "- **Audit Trail & Logging** - Security and compliance\n",
        "- **Metadata Retrieval** - Data discovery and lineage\n",
        "- **Data Quality** - Validation and monitoring\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before running this notebook, ensure you have:\n",
        "- Snowflake account with Horizon Catalog enabled\n",
        "- Appropriate permissions for catalog management\n",
        "- Access to sample data for demonstrations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import snowflake.connector\n",
        "import pandas as pd\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration - Update these with your Snowflake connection details\n",
        "CONNECTION_CONFIG = {\n",
        "    'account': 'your_account.snowflakecomputing.com',\n",
        "    'user': 'your_username',\n",
        "    'password': 'your_password',\n",
        "    'warehouse': 'COMPUTE_WH',\n",
        "    'database': 'HORIZON_CATALOG_DEMO',\n",
        "    'schema': 'PUBLIC',\n",
        "    'role': 'ACCOUNTADMIN'\n",
        "}\n",
        "\n",
        "# Initialize connection\n",
        "def get_snowflake_connection():\n",
        "    \"\"\"Establish connection to Snowflake\"\"\"\n",
        "    try:\n",
        "        conn = snowflake.connector.connect(**CONNECTION_CONFIG)\n",
        "        print(\"✅ Successfully connected to Snowflake\")\n",
        "        return conn\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Connection failed: {e}\")\n",
        "        return None\n",
        "\n",
        "# Test connection\n",
        "conn = get_snowflake_connection()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Iceberg Tables\n",
        "\n",
        "Apache Iceberg is an open table format that provides ACID transactions, schema evolution, and time travel capabilities. Snowflake Horizon Catalog supports Iceberg tables for better interoperability and performance.\n",
        "\n",
        "## Key Features:\n",
        "- **ACID Transactions**: Ensures data consistency\n",
        "- **Schema Evolution**: Add, drop, or modify columns without breaking existing queries\n",
        "- **Time Travel**: Access historical data versions\n",
        "- **Partition Evolution**: Modify partitioning schemes\n",
        "- **Multi-table Transactions**: Atomic operations across multiple tables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Iceberg Tables Examples\n",
        "\n",
        "def execute_sql(conn, sql, description=\"\"):\n",
        "    \"\"\"Execute SQL and return results as DataFrame\"\"\"\n",
        "    try:\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(sql)\n",
        "        results = cursor.fetchall()\n",
        "        columns = [desc[0] for desc in cursor.description]\n",
        "        df = pd.DataFrame(results, columns=columns)\n",
        "        print(f\"✅ {description}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error executing SQL: {e}\")\n",
        "        return None\n",
        "\n",
        "# 1. Create Iceberg Catalog\n",
        "print(\"Creating Iceberg Catalog...\")\n",
        "create_catalog_sql = \"\"\"\n",
        "CREATE CATALOG IF NOT EXISTS iceberg_catalog\n",
        "WITH (\n",
        "    CATALOG_TYPE = 'ICEBERG',\n",
        "    CATALOG_PROVIDER = 'SNOWFLAKE',\n",
        "    CATALOG_SOURCE = 'SNOWFLAKE'\n",
        ");\n",
        "\"\"\"\n",
        "execute_sql(conn, create_catalog_sql, \"Iceberg catalog created\")\n",
        "\n",
        "# 2. Create Iceberg Table\n",
        "print(\"\\nCreating Iceberg Table...\")\n",
        "create_iceberg_table_sql = \"\"\"\n",
        "CREATE TABLE iceberg_catalog.sales_data (\n",
        "    order_id STRING,\n",
        "    customer_id STRING,\n",
        "    product_id STRING,\n",
        "    order_date DATE,\n",
        "    quantity INTEGER,\n",
        "    unit_price DECIMAL(10,2),\n",
        "    total_amount DECIMAL(12,2)\n",
        ")\n",
        "USING ICEBERG\n",
        "PARTITIONED BY (order_date)\n",
        "TBLPROPERTIES (\n",
        "    'write.format.default' = 'parquet',\n",
        "    'write.parquet.compression-codec' = 'snappy'\n",
        ");\n",
        "\"\"\"\n",
        "execute_sql(conn, create_iceberg_table_sql, \"Iceberg table created\")\n",
        "\n",
        "# 3. Insert sample data\n",
        "print(\"\\nInserting sample data...\")\n",
        "insert_data_sql = \"\"\"\n",
        "INSERT INTO iceberg_catalog.sales_data VALUES\n",
        "('ORD001', 'CUST001', 'PROD001', '2024-01-15', 2, 25.50, 51.00),\n",
        "('ORD002', 'CUST002', 'PROD002', '2024-01-16', 1, 45.00, 45.00),\n",
        "('ORD003', 'CUST001', 'PROD003', '2024-01-17', 3, 15.75, 47.25),\n",
        "('ORD004', 'CUST003', 'PROD001', '2024-01-18', 1, 25.50, 25.50),\n",
        "('ORD005', 'CUST002', 'PROD004', '2024-01-19', 2, 30.00, 60.00);\n",
        "\"\"\"\n",
        "execute_sql(conn, insert_data_sql, \"Sample data inserted\")\n",
        "\n",
        "# 4. Query Iceberg table\n",
        "print(\"\\nQuerying Iceberg table...\")\n",
        "query_sql = \"SELECT * FROM iceberg_catalog.sales_data ORDER BY order_date;\"\n",
        "df_sales = execute_sql(conn, query_sql, \"Data retrieved from Iceberg table\")\n",
        "if df_sales is not None:\n",
        "    print(df_sales)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Iceberg Advanced Features\n",
        "\n",
        "# 5. Schema Evolution - Add new column\n",
        "print(\"Demonstrating Schema Evolution...\")\n",
        "alter_table_sql = \"\"\"\n",
        "ALTER TABLE iceberg_catalog.sales_data \n",
        "ADD COLUMN discount_percent DECIMAL(5,2) DEFAULT 0.0;\n",
        "\"\"\"\n",
        "execute_sql(conn, alter_table_sql, \"Added discount_percent column\")\n",
        "\n",
        "# 6. Update data with new column\n",
        "update_sql = \"\"\"\n",
        "UPDATE iceberg_catalog.sales_data \n",
        "SET discount_percent = 10.0 \n",
        "WHERE customer_id = 'CUST001';\n",
        "\"\"\"\n",
        "execute_sql(conn, update_sql, \"Updated data with discount\")\n",
        "\n",
        "# 7. Time Travel - Show table history\n",
        "print(\"\\nTime Travel - Table History:\")\n",
        "history_sql = \"\"\"\n",
        "SELECT \n",
        "    snapshot_id,\n",
        "    committed_at,\n",
        "    operation,\n",
        "    summary\n",
        "FROM iceberg_catalog.sales_data.snapshots\n",
        "ORDER BY committed_at DESC;\n",
        "\"\"\"\n",
        "df_history = execute_sql(conn, history_sql, \"Retrieved table history\")\n",
        "if df_history is not None:\n",
        "    print(df_history)\n",
        "\n",
        "# 8. Partition Evolution\n",
        "print(\"\\nPartition Evolution:\")\n",
        "partition_info_sql = \"\"\"\n",
        "SELECT \n",
        "    partition_spec_id,\n",
        "    partition_fields,\n",
        "    partition_values\n",
        "FROM iceberg_catalog.sales_data.partitions\n",
        "ORDER BY partition_spec_id;\n",
        "\"\"\"\n",
        "df_partitions = execute_sql(conn, partition_info_sql, \"Retrieved partition information\")\n",
        "if df_partitions is not None:\n",
        "    print(df_partitions)\n",
        "\n",
        "# 9. Table Properties and Metadata\n",
        "print(\"\\nTable Properties:\")\n",
        "properties_sql = \"\"\"\n",
        "SHOW TABLE PROPERTIES iceberg_catalog.sales_data;\n",
        "\"\"\"\n",
        "execute_sql(conn, properties_sql, \"Retrieved table properties\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Traditional Tables & Views\n",
        "\n",
        "Snowflake's traditional tables and views provide the foundation for data storage and access patterns. Horizon Catalog enhances these with improved metadata management and governance capabilities.\n",
        "\n",
        "## Table Types:\n",
        "- **Permanent Tables**: Standard persistent storage\n",
        "- **Temporary Tables**: Session-scoped storage\n",
        "- **Transient Tables**: Short-term storage with reduced durability\n",
        "- **External Tables**: Reference external data sources\n",
        "- **Materialized Views**: Pre-computed query results\n",
        "- **Dynamic Tables**: Automated data pipelines\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Traditional Tables & Views Examples\n",
        "\n",
        "# 1. Create different types of tables\n",
        "print(\"Creating Traditional Tables...\")\n",
        "\n",
        "# Permanent Table\n",
        "create_permanent_table_sql = \"\"\"\n",
        "CREATE OR REPLACE TABLE customers (\n",
        "    customer_id STRING PRIMARY KEY,\n",
        "    first_name STRING,\n",
        "    last_name STRING,\n",
        "    email STRING,\n",
        "    phone STRING,\n",
        "    address STRING,\n",
        "    city STRING,\n",
        "    state STRING,\n",
        "    zip_code STRING,\n",
        "    created_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),\n",
        "    updated_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n",
        ");\n",
        "\"\"\"\n",
        "execute_sql(conn, create_permanent_table_sql, \"Permanent table created\")\n",
        "\n",
        "# Temporary Table\n",
        "create_temp_table_sql = \"\"\"\n",
        "CREATE OR REPLACE TEMPORARY TABLE temp_orders (\n",
        "    order_id STRING,\n",
        "    customer_id STRING,\n",
        "    order_date DATE,\n",
        "    status STRING\n",
        ");\n",
        "\"\"\"\n",
        "execute_sql(conn, create_temp_table_sql, \"Temporary table created\")\n",
        "\n",
        "# Transient Table\n",
        "create_transient_table_sql = \"\"\"\n",
        "CREATE OR REPLACE TRANSIENT TABLE staging_products (\n",
        "    product_id STRING,\n",
        "    product_name STRING,\n",
        "    category STRING,\n",
        "    price DECIMAL(10,2),\n",
        "    stock_quantity INTEGER\n",
        ");\n",
        "\"\"\"\n",
        "execute_sql(conn, create_transient_table_sql, \"Transient table created\")\n",
        "\n",
        "# 2. Insert sample data\n",
        "print(\"\\nInserting sample data...\")\n",
        "\n",
        "# Insert into customers table\n",
        "insert_customers_sql = \"\"\"\n",
        "INSERT INTO customers VALUES\n",
        "('CUST001', 'John', 'Doe', 'john.doe@email.com', '555-0101', '123 Main St', 'New York', 'NY', '10001', CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP()),\n",
        "('CUST002', 'Jane', 'Smith', 'jane.smith@email.com', '555-0102', '456 Oak Ave', 'Los Angeles', 'CA', '90210', CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP()),\n",
        "('CUST003', 'Bob', 'Johnson', 'bob.johnson@email.com', '555-0103', '789 Pine Rd', 'Chicago', 'IL', '60601', CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP());\n",
        "\"\"\"\n",
        "execute_sql(conn, insert_customers_sql, \"Customer data inserted\")\n",
        "\n",
        "# Insert into staging products\n",
        "insert_products_sql = \"\"\"\n",
        "INSERT INTO staging_products VALUES\n",
        "('PROD001', 'Laptop Computer', 'Electronics', 999.99, 50),\n",
        "('PROD002', 'Office Chair', 'Furniture', 299.99, 25),\n",
        "('PROD003', 'Coffee Mug', 'Kitchen', 12.99, 100),\n",
        "('PROD004', 'Desk Lamp', 'Office', 45.99, 75);\n",
        "\"\"\"\n",
        "execute_sql(conn, insert_products_sql, \"Product data inserted\")\n",
        "\n",
        "# 3. Create Views\n",
        "print(\"\\nCreating Views...\")\n",
        "\n",
        "# Standard View\n",
        "create_view_sql = \"\"\"\n",
        "CREATE OR REPLACE VIEW customer_summary AS\n",
        "SELECT \n",
        "    customer_id,\n",
        "    CONCAT(first_name, ' ', last_name) AS full_name,\n",
        "    email,\n",
        "    city,\n",
        "    state,\n",
        "    created_date\n",
        "FROM customers\n",
        "WHERE created_date >= DATEADD(day, -30, CURRENT_DATE());\n",
        "\"\"\"\n",
        "execute_sql(conn, create_view_sql, \"Standard view created\")\n",
        "\n",
        "# Materialized View\n",
        "create_materialized_view_sql = \"\"\"\n",
        "CREATE OR REPLACE MATERIALIZED VIEW product_summary AS\n",
        "SELECT \n",
        "    category,\n",
        "    COUNT(*) AS product_count,\n",
        "    AVG(price) AS avg_price,\n",
        "    SUM(stock_quantity) AS total_stock\n",
        "FROM staging_products\n",
        "GROUP BY category;\n",
        "\"\"\"\n",
        "execute_sql(conn, create_materialized_view_sql, \"Materialized view created\")\n",
        "\n",
        "# 4. Query the views\n",
        "print(\"\\nQuerying Views...\")\n",
        "view_query_sql = \"SELECT * FROM customer_summary;\"\n",
        "df_customers = execute_sql(conn, view_query_sql, \"Retrieved customer summary\")\n",
        "if df_customers is not None:\n",
        "    print(\"Customer Summary:\")\n",
        "    print(df_customers)\n",
        "\n",
        "materialized_query_sql = \"SELECT * FROM product_summary;\"\n",
        "df_products = execute_sql(conn, materialized_query_sql, \"Retrieved product summary\")\n",
        "if df_products is not None:\n",
        "    print(\"\\nProduct Summary:\")\n",
        "    print(df_products)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dynamic Tables and Advanced Features\n",
        "\n",
        "# 5. Create Dynamic Table (Automated Data Pipeline)\n",
        "print(\"\\nCreating Dynamic Table...\")\n",
        "create_dynamic_table_sql = \"\"\"\n",
        "CREATE OR REPLACE DYNAMIC TABLE customer_analytics\n",
        "TARGET_LAG = '1 hour'\n",
        "WAREHOUSE = 'COMPUTE_WH'\n",
        "AS\n",
        "SELECT \n",
        "    c.customer_id,\n",
        "    c.first_name,\n",
        "    c.last_name,\n",
        "    c.city,\n",
        "    c.state,\n",
        "    COUNT(s.order_id) AS total_orders,\n",
        "    SUM(s.total_amount) AS total_spent,\n",
        "    AVG(s.total_amount) AS avg_order_value,\n",
        "    MAX(s.order_date) AS last_order_date\n",
        "FROM customers c\n",
        "LEFT JOIN iceberg_catalog.sales_data s ON c.customer_id = s.customer_id\n",
        "GROUP BY c.customer_id, c.first_name, c.last_name, c.city, c.state;\n",
        "\"\"\"\n",
        "execute_sql(conn, create_dynamic_table_sql, \"Dynamic table created\")\n",
        "\n",
        "# 6. External Table (Reference external data)\n",
        "print(\"\\nCreating External Table...\")\n",
        "create_external_table_sql = \"\"\"\n",
        "CREATE OR REPLACE EXTERNAL TABLE external_weather_data (\n",
        "    date DATE AS (value:c1::DATE),\n",
        "    temperature FLOAT AS (value:c2::FLOAT),\n",
        "    humidity INTEGER AS (value:c3::INTEGER),\n",
        "    city STRING AS (value:c4::STRING)\n",
        ")\n",
        "LOCATION = @my_stage/weather/\n",
        "FILE_FORMAT = (TYPE = 'CSV' FIELD_DELIMITER = ',' SKIP_HEADER = 1);\n",
        "\"\"\"\n",
        "execute_sql(conn, create_external_table_sql, \"External table created\")\n",
        "\n",
        "# 7. Table Cloning\n",
        "print(\"\\nDemonstrating Table Cloning...\")\n",
        "clone_table_sql = \"\"\"\n",
        "CREATE OR REPLACE TABLE customers_backup CLONE customers;\n",
        "\"\"\"\n",
        "execute_sql(conn, clone_table_sql, \"Table cloned\")\n",
        "\n",
        "# 8. Show table information\n",
        "print(\"\\nTable Information:\")\n",
        "show_tables_sql = \"SHOW TABLES;\"\n",
        "df_tables = execute_sql(conn, show_tables_sql, \"Retrieved table list\")\n",
        "if df_tables is not None:\n",
        "    print(df_tables)\n",
        "\n",
        "# 9. Table Statistics\n",
        "print(\"\\nTable Statistics:\")\n",
        "stats_sql = \"\"\"\n",
        "SELECT \n",
        "    table_name,\n",
        "    row_count,\n",
        "    bytes,\n",
        "    created,\n",
        "    last_altered\n",
        "FROM information_schema.tables \n",
        "WHERE table_schema = 'PUBLIC'\n",
        "ORDER BY created DESC;\n",
        "\"\"\"\n",
        "df_stats = execute_sql(conn, stats_sql, \"Retrieved table statistics\")\n",
        "if df_stats is not None:\n",
        "    print(df_stats)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. AI & ML Objects\n",
        "\n",
        "Snowflake Horizon Catalog provides comprehensive support for AI and Machine Learning objects, enabling organizations to manage ML models, functions, and AI-powered analytics within their data platform.\n",
        "\n",
        "## AI/ML Object Types:\n",
        "- **ML Models**: Trained machine learning models\n",
        "- **ML Functions**: Custom ML functions and procedures\n",
        "- **Vector Embeddings**: Semantic search capabilities\n",
        "- **AI Functions**: Built-in AI functions for text analysis\n",
        "- **ML Pipelines**: Automated ML workflows\n",
        "- **Model Registry**: Centralized model management\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AI & ML Objects Examples\n",
        "\n",
        "# 1. Create ML Model Registry\n",
        "print(\"Creating ML Model Registry...\")\n",
        "create_model_registry_sql = \"\"\"\n",
        "CREATE OR REPLACE SCHEMA ml_models;\n",
        "\"\"\"\n",
        "execute_sql(conn, create_model_registry_sql, \"ML schema created\")\n",
        "\n",
        "# 2. Create ML Model Table\n",
        "print(\"\\nCreating ML Model Storage...\")\n",
        "create_model_table_sql = \"\"\"\n",
        "CREATE OR REPLACE TABLE ml_models.model_registry (\n",
        "    model_id STRING PRIMARY KEY,\n",
        "    model_name STRING,\n",
        "    model_version STRING,\n",
        "    model_type STRING,\n",
        "    training_data STRING,\n",
        "    model_metrics VARIANT,\n",
        "    model_artifact STRING,\n",
        "    created_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),\n",
        "    created_by STRING,\n",
        "    status STRING DEFAULT 'ACTIVE',\n",
        "    description STRING\n",
        ");\n",
        "\"\"\"\n",
        "execute_sql(conn, create_model_table_sql, \"Model registry table created\")\n",
        "\n",
        "# 3. Insert Sample ML Models\n",
        "print(\"\\nRegistering Sample ML Models...\")\n",
        "insert_models_sql = \"\"\"\n",
        "INSERT INTO ml_models.model_registry VALUES\n",
        "('MODEL001', 'customer_churn_prediction', 'v1.0', 'CLASSIFICATION', 'customers, sales_data', \n",
        " PARSE_JSON('{\"accuracy\": 0.89, \"precision\": 0.87, \"recall\": 0.91, \"f1_score\": 0.89}'),\n",
        " 's3://ml-artifacts/churn_model_v1.pkl', CURRENT_TIMESTAMP(), 'data_scientist', 'ACTIVE',\n",
        " 'Predicts customer churn probability based on purchase history and demographics'),\n",
        " \n",
        "('MODEL002', 'sales_forecasting', 'v2.1', 'REGRESSION', 'sales_data, product_data',\n",
        " PARSE_JSON('{\"rmse\": 125.5, \"mae\": 98.2, \"r2_score\": 0.85}'),\n",
        " 's3://ml-artifacts/sales_forecast_v2.pkl', CURRENT_TIMESTAMP(), 'ml_engineer', 'ACTIVE',\n",
        " 'Forecasts sales revenue for next quarter based on historical trends'),\n",
        " \n",
        "('MODEL003', 'product_recommendation', 'v1.5', 'RECOMMENDATION', 'customers, sales_data, products',\n",
        " PARSE_JSON('{\"hit_rate\": 0.76, \"ndcg\": 0.82, \"coverage\": 0.91}'),\n",
        " 's3://ml-artifacts/recommendation_v1.pkl', CURRENT_TIMESTAMP(), 'data_scientist', 'ACTIVE',\n",
        " 'Recommends products to customers based on purchase history and preferences');\n",
        "\"\"\"\n",
        "execute_sql(conn, insert_models_sql, \"ML models registered\")\n",
        "\n",
        "# 4. Create ML Function\n",
        "print(\"\\nCreating ML Function...\")\n",
        "create_ml_function_sql = \"\"\"\n",
        "CREATE OR REPLACE FUNCTION ml_models.predict_churn(customer_id STRING)\n",
        "RETURNS FLOAT\n",
        "LANGUAGE SQL\n",
        "AS\n",
        "$$\n",
        "    -- This is a simplified example - in practice, you would call your actual ML model\n",
        "    SELECT \n",
        "        CASE \n",
        "            WHEN total_orders < 2 THEN 0.8\n",
        "            WHEN total_spent < 100 THEN 0.6\n",
        "            WHEN days_since_last_order > 90 THEN 0.7\n",
        "            ELSE 0.2\n",
        "        END as churn_probability\n",
        "    FROM (\n",
        "        SELECT \n",
        "            COUNT(s.order_id) as total_orders,\n",
        "            SUM(s.total_amount) as total_spent,\n",
        "            DATEDIFF(day, MAX(s.order_date), CURRENT_DATE()) as days_since_last_order\n",
        "        FROM customers c\n",
        "        LEFT JOIN iceberg_catalog.sales_data s ON c.customer_id = s.customer_id\n",
        "        WHERE c.customer_id = customer_id\n",
        "        GROUP BY c.customer_id\n",
        "    );\n",
        "$$;\n",
        "\"\"\"\n",
        "execute_sql(conn, create_ml_function_sql, \"ML function created\")\n",
        "\n",
        "# 5. Test ML Function\n",
        "print(\"\\nTesting ML Function...\")\n",
        "test_ml_function_sql = \"\"\"\n",
        "SELECT \n",
        "    customer_id,\n",
        "    first_name,\n",
        "    last_name,\n",
        "    ml_models.predict_churn(customer_id) as churn_probability\n",
        "FROM customers\n",
        "ORDER BY churn_probability DESC;\n",
        "\"\"\"\n",
        "df_churn_prediction = execute_sql(conn, test_ml_function_sql, \"Churn prediction completed\")\n",
        "if df_churn_prediction is not None:\n",
        "    print(\"Churn Prediction Results:\")\n",
        "    print(df_churn_prediction)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AI Functions and Vector Embeddings\n",
        "\n",
        "# 6. Create AI Functions for Text Analysis\n",
        "print(\"\\nCreating AI Functions...\")\n",
        "create_ai_function_sql = \"\"\"\n",
        "CREATE OR REPLACE FUNCTION ml_models.analyze_sentiment(text STRING)\n",
        "RETURNS STRING\n",
        "LANGUAGE SQL\n",
        "AS\n",
        "$$\n",
        "    -- Simplified sentiment analysis function\n",
        "    SELECT \n",
        "        CASE \n",
        "            WHEN UPPER(text) LIKE '%EXCELLENT%' OR UPPER(text) LIKE '%GREAT%' THEN 'POSITIVE'\n",
        "            WHEN UPPER(text) LIKE '%TERRIBLE%' OR UPPER(text) LIKE '%BAD%' THEN 'NEGATIVE'\n",
        "            ELSE 'NEUTRAL'\n",
        "        END;\n",
        "$$;\n",
        "\"\"\"\n",
        "execute_sql(conn, create_ai_function_sql, \"AI function created\")\n",
        "\n",
        "# 7. Create Vector Embeddings Table\n",
        "print(\"\\nCreating Vector Embeddings...\")\n",
        "create_embeddings_sql = \"\"\"\n",
        "CREATE OR REPLACE TABLE ml_models.product_embeddings (\n",
        "    product_id STRING,\n",
        "    product_name STRING,\n",
        "    embedding VECTOR(FLOAT, 384),\n",
        "    created_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n",
        ");\n",
        "\"\"\"\n",
        "execute_sql(conn, create_embeddings_sql, \"Vector embeddings table created\")\n",
        "\n",
        "# 8. Model Performance Monitoring\n",
        "print(\"\\nModel Performance Monitoring...\")\n",
        "performance_sql = \"\"\"\n",
        "SELECT \n",
        "    model_name,\n",
        "    model_version,\n",
        "    model_metrics,\n",
        "    created_date,\n",
        "    status\n",
        "FROM ml_models.model_registry\n",
        "WHERE status = 'ACTIVE'\n",
        "ORDER BY created_date DESC;\n",
        "\"\"\"\n",
        "df_performance = execute_sql(conn, performance_sql, \"Model performance retrieved\")\n",
        "if df_performance is not None:\n",
        "    print(df_performance)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Costs & Usage Monitoring\n",
        "\n",
        "Horizon Catalog provides comprehensive cost monitoring and usage analytics to help organizations optimize their Snowflake spending and resource utilization.\n",
        "\n",
        "## Key Features:\n",
        "- **Cost Tracking**: Monitor compute and storage costs\n",
        "- **Usage Analytics**: Analyze resource consumption patterns\n",
        "- **Budget Alerts**: Set up cost thresholds and notifications\n",
        "- **Resource Optimization**: Identify optimization opportunities\n",
        "- **Chargeback Reporting**: Allocate costs to departments/projects\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cost and Usage Monitoring Examples\n",
        "\n",
        "# 1. Create Cost Tracking Tables\n",
        "print(\"Setting up Cost Monitoring...\")\n",
        "create_cost_tracking_sql = \"\"\"\n",
        "CREATE OR REPLACE SCHEMA cost_monitoring;\n",
        "\"\"\"\n",
        "execute_sql(conn, create_cost_tracking_sql, \"Cost monitoring schema created\")\n",
        "\n",
        "# 2. Warehouse Usage Tracking\n",
        "print(\"\\nWarehouse Usage Analysis...\")\n",
        "warehouse_usage_sql = \"\"\"\n",
        "SELECT \n",
        "    warehouse_name,\n",
        "    DATE(start_time) as usage_date,\n",
        "    COUNT(*) as query_count,\n",
        "    SUM(credits_used) as total_credits,\n",
        "    AVG(credits_used) as avg_credits_per_query,\n",
        "    SUM(total_elapsed_time) as total_time_ms\n",
        "FROM snowflake.account_usage.warehouse_metering_history\n",
        "WHERE start_time >= DATEADD(day, -7, CURRENT_DATE())\n",
        "GROUP BY warehouse_name, DATE(start_time)\n",
        "ORDER BY usage_date DESC, total_credits DESC;\n",
        "\"\"\"\n",
        "df_warehouse_usage = execute_sql(conn, warehouse_usage_sql, \"Warehouse usage retrieved\")\n",
        "\n",
        "# 3. Storage Cost Analysis\n",
        "print(\"\\nStorage Cost Analysis...\")\n",
        "storage_cost_sql = \"\"\"\n",
        "SELECT \n",
        "    database_name,\n",
        "    schema_name,\n",
        "    table_name,\n",
        "    ROUND(bytes / 1024 / 1024 / 1024, 2) as size_gb,\n",
        "    ROUND(bytes / 1024 / 1024 / 1024 * 0.023, 2) as estimated_monthly_cost_usd,\n",
        "    row_count,\n",
        "    created,\n",
        "    last_altered\n",
        "FROM snowflake.information_schema.tables\n",
        "WHERE table_schema NOT IN ('INFORMATION_SCHEMA')\n",
        "ORDER BY bytes DESC\n",
        "LIMIT 10;\n",
        "\"\"\"\n",
        "df_storage_cost = execute_sql(conn, storage_cost_sql, \"Storage cost analysis completed\")\n",
        "\n",
        "# 4. Query Performance and Cost\n",
        "print(\"\\nQuery Performance Analysis...\")\n",
        "query_performance_sql = \"\"\"\n",
        "SELECT \n",
        "    query_id,\n",
        "    query_text,\n",
        "    warehouse_name,\n",
        "    credits_used,\n",
        "    total_elapsed_time,\n",
        "    bytes_scanned,\n",
        "    rows_produced,\n",
        "    DATE(start_time) as query_date\n",
        "FROM snowflake.account_usage.query_history\n",
        "WHERE start_time >= DATEADD(day, -1, CURRENT_DATE())\n",
        "  AND credits_used > 0.1\n",
        "ORDER BY credits_used DESC\n",
        "LIMIT 10;\n",
        "\"\"\"\n",
        "df_query_performance = execute_sql(conn, query_performance_sql, \"Query performance analysis completed\")\n",
        "\n",
        "# 5. Cost Optimization Recommendations\n",
        "print(\"\\nCost Optimization Analysis...\")\n",
        "optimization_sql = \"\"\"\n",
        "SELECT \n",
        "    'Large Tables' as category,\n",
        "    COUNT(*) as count,\n",
        "    SUM(bytes) as total_bytes,\n",
        "    ROUND(SUM(bytes) / 1024 / 1024 / 1024, 2) as total_gb\n",
        "FROM snowflake.information_schema.tables\n",
        "WHERE bytes > 1024 * 1024 * 1024  -- Tables larger than 1GB\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT \n",
        "    'Unused Tables' as category,\n",
        "    COUNT(*) as count,\n",
        "    SUM(bytes) as total_bytes,\n",
        "    ROUND(SUM(bytes) / 1024 / 1024 / 1024, 2) as total_gb\n",
        "FROM snowflake.information_schema.tables t\n",
        "LEFT JOIN snowflake.account_usage.access_history ah \n",
        "    ON t.table_name = ah.object_name\n",
        "WHERE ah.object_name IS NULL\n",
        "  AND t.created < DATEADD(day, -30, CURRENT_DATE());\n",
        "\"\"\"\n",
        "df_optimization = execute_sql(conn, optimization_sql, \"Cost optimization analysis completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Data Warehouses Management\n",
        "\n",
        "Data warehouses are compute resources that execute SQL queries. Horizon Catalog provides enhanced management capabilities for optimizing warehouse performance and costs.\n",
        "\n",
        "## Warehouse Features:\n",
        "- **Multi-cluster Warehouses**: Auto-scaling compute resources\n",
        "- **Warehouse Sizing**: Right-size compute for workloads\n",
        "- **Auto-suspend/Auto-resume**: Cost optimization\n",
        "- **Resource Monitors**: Credit usage limits\n",
        "- **Query Optimization**: Performance tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Warehouse Management Examples\n",
        "\n",
        "# 1. Create Optimized Warehouse\n",
        "print(\"Creating Optimized Warehouse...\")\n",
        "create_warehouse_sql = \"\"\"\n",
        "CREATE OR REPLACE WAREHOUSE analytics_wh\n",
        "WITH \n",
        "    WAREHOUSE_SIZE = 'MEDIUM'\n",
        "    AUTO_SUSPEND = 300\n",
        "    AUTO_RESUME = TRUE\n",
        "    MIN_CLUSTER_COUNT = 1\n",
        "    MAX_CLUSTER_COUNT = 3\n",
        "    SCALING_POLICY = 'STANDARD'\n",
        "    COMMENT = 'Analytics warehouse with auto-scaling';\n",
        "\"\"\"\n",
        "execute_sql(conn, create_warehouse_sql, \"Analytics warehouse created\")\n",
        "\n",
        "# 2. Warehouse Performance Analysis\n",
        "print(\"\\nWarehouse Performance Analysis...\")\n",
        "warehouse_performance_sql = \"\"\"\n",
        "SELECT \n",
        "    warehouse_name,\n",
        "    AVG(credits_used) as avg_credits,\n",
        "    MAX(credits_used) as max_credits,\n",
        "    COUNT(*) as query_count,\n",
        "    AVG(total_elapsed_time) as avg_execution_time_ms\n",
        "FROM snowflake.account_usage.query_history\n",
        "WHERE start_time >= DATEADD(day, -7, CURRENT_DATE())\n",
        "GROUP BY warehouse_name\n",
        "ORDER BY avg_credits DESC;\n",
        "\"\"\"\n",
        "df_warehouse_perf = execute_sql(conn, warehouse_performance_sql, \"Warehouse performance analyzed\")\n",
        "\n",
        "# 3. Resource Monitor Setup\n",
        "print(\"\\nSetting up Resource Monitor...\")\n",
        "resource_monitor_sql = \"\"\"\n",
        "CREATE OR REPLACE RESOURCE MONITOR monthly_budget\n",
        "WITH \n",
        "    CREDIT_QUOTA = 1000\n",
        "    FREQUENCY = MONTHLY\n",
        "    START_TIMESTAMP = CURRENT_TIMESTAMP()\n",
        "    TRIGGERS\n",
        "        (75 PERCENT DO NOTIFY,\n",
        "         90 PERCENT DO SUSPEND,\n",
        "         100 PERCENT DO SUSPEND_IMMEDIATE);\n",
        "\"\"\"\n",
        "execute_sql(conn, resource_monitor_sql, \"Resource monitor created\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. Data Shares\n",
        "\n",
        "Data sharing enables secure, real-time data sharing between Snowflake accounts without copying data. Horizon Catalog enhances sharing with improved governance and monitoring.\n",
        "\n",
        "## Sharing Features:\n",
        "- **Secure Sharing**: No data movement required\n",
        "- **Real-time Access**: Live data access\n",
        "- **Governance Controls**: Fine-grained permissions\n",
        "- **Usage Monitoring**: Track shared data consumption\n",
        "- **Cross-cloud Sharing**: Share across cloud providers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Sharing Examples\n",
        "\n",
        "# 1. Create Data Share\n",
        "print(\"Creating Data Share...\")\n",
        "create_share_sql = \"\"\"\n",
        "CREATE OR REPLACE SHARE customer_analytics_share\n",
        "COMMENT = 'Customer analytics data for business partners';\n",
        "\"\"\"\n",
        "execute_sql(conn, create_share_sql, \"Data share created\")\n",
        "\n",
        "# 2. Add Objects to Share\n",
        "print(\"\\nAdding Objects to Share...\")\n",
        "add_to_share_sql = \"\"\"\n",
        "GRANT USAGE ON DATABASE HORIZON_CATALOG_DEMO TO SHARE customer_analytics_share;\n",
        "GRANT USAGE ON SCHEMA HORIZON_CATALOG_DEMO.PUBLIC TO SHARE customer_analytics_share;\n",
        "GRANT SELECT ON TABLE HORIZON_CATALOG_DEMO.PUBLIC.customers TO SHARE customer_analytics_share;\n",
        "GRANT SELECT ON VIEW HORIZON_CATALOG_DEMO.PUBLIC.customer_summary TO SHARE customer_analytics_share;\n",
        "\"\"\"\n",
        "execute_sql(conn, add_to_share_sql, \"Objects added to share\")\n",
        "\n",
        "# 3. Share Usage Monitoring\n",
        "print(\"\\nShare Usage Monitoring...\")\n",
        "share_usage_sql = \"\"\"\n",
        "SELECT \n",
        "    share_name,\n",
        "    consumer_account,\n",
        "    consumer_account_locator,\n",
        "    created_on,\n",
        "    kind\n",
        "FROM snowflake.account_usage.shares\n",
        "WHERE share_name = 'CUSTOMER_ANALYTICS_SHARE';\n",
        "\"\"\"\n",
        "df_share_usage = execute_sql(conn, share_usage_sql, \"Share usage monitored\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7. Semantic Views\n",
        "\n",
        "Semantic views provide business-friendly abstractions over technical data structures, making data more accessible to business users and analytics tools.\n",
        "\n",
        "## Semantic View Features:\n",
        "- **Business Logic**: Encapsulate complex business rules\n",
        "- **Data Abstraction**: Hide technical complexity\n",
        "- **Consistent Metrics**: Standardized calculations\n",
        "- **Self-Service Analytics**: Enable business users\n",
        "- **Governance**: Centralized business definitions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Semantic Views Examples\n",
        "\n",
        "# 1. Create Business Metrics View\n",
        "print(\"Creating Semantic Views...\")\n",
        "create_semantic_view_sql = \"\"\"\n",
        "CREATE OR REPLACE VIEW business_metrics.customer_kpi AS\n",
        "SELECT \n",
        "    c.customer_id,\n",
        "    CONCAT(c.first_name, ' ', c.last_name) AS customer_name,\n",
        "    c.city,\n",
        "    c.state,\n",
        "    COUNT(s.order_id) AS total_orders,\n",
        "    SUM(s.total_amount) AS lifetime_value,\n",
        "    AVG(s.total_amount) AS avg_order_value,\n",
        "    MAX(s.order_date) AS last_purchase_date,\n",
        "    DATEDIFF(day, MAX(s.order_date), CURRENT_DATE()) AS days_since_last_purchase,\n",
        "    CASE \n",
        "        WHEN COUNT(s.order_id) >= 10 THEN 'VIP'\n",
        "        WHEN COUNT(s.order_id) >= 5 THEN 'Premium'\n",
        "        WHEN COUNT(s.order_id) >= 2 THEN 'Regular'\n",
        "        ELSE 'New'\n",
        "    END AS customer_tier,\n",
        "    CASE \n",
        "        WHEN DATEDIFF(day, MAX(s.order_date), CURRENT_DATE()) <= 30 THEN 'Active'\n",
        "        WHEN DATEDIFF(day, MAX(s.order_date), CURRENT_DATE()) <= 90 THEN 'At Risk'\n",
        "        ELSE 'Inactive'\n",
        "    END AS customer_status\n",
        "FROM customers c\n",
        "LEFT JOIN iceberg_catalog.sales_data s ON c.customer_id = s.customer_id\n",
        "GROUP BY c.customer_id, c.first_name, c.last_name, c.city, c.state;\n",
        "\"\"\"\n",
        "execute_sql(conn, create_semantic_view_sql, \"Semantic view created\")\n",
        "\n",
        "# 2. Revenue Analytics View\n",
        "print(\"\\nCreating Revenue Analytics View...\")\n",
        "revenue_view_sql = \"\"\"\n",
        "CREATE OR REPLACE VIEW business_metrics.revenue_analytics AS\n",
        "SELECT \n",
        "    DATE_TRUNC('month', order_date) AS month,\n",
        "    COUNT(DISTINCT customer_id) AS active_customers,\n",
        "    COUNT(order_id) AS total_orders,\n",
        "    SUM(total_amount) AS total_revenue,\n",
        "    AVG(total_amount) AS avg_order_value,\n",
        "    SUM(total_amount) / COUNT(DISTINCT customer_id) AS revenue_per_customer\n",
        "FROM iceberg_catalog.sales_data\n",
        "GROUP BY DATE_TRUNC('month', order_date)\n",
        "ORDER BY month;\n",
        "\"\"\"\n",
        "execute_sql(conn, revenue_view_sql, \"Revenue analytics view created\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8. Databases & Schemas\n",
        "\n",
        "Databases and schemas provide logical organization for data objects. Horizon Catalog enhances this with improved metadata management and governance.\n",
        "\n",
        "## Organization Features:\n",
        "- **Logical Grouping**: Organize related objects\n",
        "- **Namespace Management**: Avoid naming conflicts\n",
        "- **Access Control**: Schema-level permissions\n",
        "- **Data Classification**: Tag and categorize data\n",
        "- **Lifecycle Management**: Automated data retention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Database and Schema Management\n",
        "\n",
        "# 1. Create Organized Database Structure\n",
        "print(\"Creating Database Structure...\")\n",
        "create_database_sql = \"\"\"\n",
        "CREATE OR REPLACE DATABASE analytics_db\n",
        "COMMENT = 'Analytics database for business intelligence';\n",
        "\"\"\"\n",
        "execute_sql(conn, create_database_sql, \"Analytics database created\")\n",
        "\n",
        "# 2. Create Organized Schemas\n",
        "print(\"\\nCreating Organized Schemas...\")\n",
        "create_schemas_sql = \"\"\"\n",
        "CREATE OR REPLACE SCHEMA analytics_db.raw_data\n",
        "COMMENT = 'Raw data ingestion layer';\n",
        "\n",
        "CREATE OR REPLACE SCHEMA analytics_db.staging\n",
        "COMMENT = 'Data staging and transformation layer';\n",
        "\n",
        "CREATE OR REPLACE SCHEMA analytics_db.mart\n",
        "COMMENT = 'Data mart for business users';\n",
        "\n",
        "CREATE OR REPLACE SCHEMA analytics_db.ml_models\n",
        "COMMENT = 'Machine learning models and functions';\n",
        "\"\"\"\n",
        "execute_sql(conn, create_schemas_sql, \"Schemas created\")\n",
        "\n",
        "# 3. Data Classification and Tagging\n",
        "print(\"\\nSetting up Data Classification...\")\n",
        "classification_sql = \"\"\"\n",
        "CREATE OR REPLACE TAG analytics_db.data_classification (\n",
        "    sensitivity_level STRING,\n",
        "    data_owner STRING,\n",
        "    retention_period INTEGER,\n",
        "    compliance_standard STRING\n",
        ");\n",
        "\"\"\"\n",
        "execute_sql(conn, classification_sql, \"Data classification tag created\")\n",
        "\n",
        "# 4. Schema Usage Analysis\n",
        "print(\"\\nSchema Usage Analysis...\")\n",
        "schema_usage_sql = \"\"\"\n",
        "SELECT \n",
        "    database_name,\n",
        "    schema_name,\n",
        "    COUNT(*) as object_count,\n",
        "    SUM(bytes) as total_bytes\n",
        "FROM snowflake.information_schema.tables\n",
        "WHERE database_name = 'ANALYTICS_DB'\n",
        "GROUP BY database_name, schema_name\n",
        "ORDER BY total_bytes DESC;\n",
        "\"\"\"\n",
        "df_schema_usage = execute_sql(conn, schema_usage_sql, \"Schema usage analyzed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 9. Business Glossary\n",
        "\n",
        "The Business Glossary provides a centralized repository for business terms, definitions, and data lineage, enabling better data governance and understanding.\n",
        "\n",
        "## Glossary Features:\n",
        "- **Term Definitions**: Centralized business vocabulary\n",
        "- **Data Lineage**: Track data flow and transformations\n",
        "- **Ownership**: Assign data stewards and owners\n",
        "- **Relationships**: Link related terms and concepts\n",
        "- **Compliance**: Document regulatory requirements\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Business Glossary Examples\n",
        "\n",
        "# 1. Create Business Glossary Tables\n",
        "print(\"Creating Business Glossary...\")\n",
        "create_glossary_sql = \"\"\"\n",
        "CREATE OR REPLACE SCHEMA business_glossary;\n",
        "\n",
        "CREATE OR REPLACE TABLE business_glossary.terms (\n",
        "    term_id STRING PRIMARY KEY,\n",
        "    term_name STRING,\n",
        "    definition STRING,\n",
        "    category STRING,\n",
        "    data_steward STRING,\n",
        "    business_owner STRING,\n",
        "    created_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),\n",
        "    updated_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),\n",
        "    status STRING DEFAULT 'ACTIVE'\n",
        ");\n",
        "\n",
        "CREATE OR REPLACE TABLE business_glossary.data_lineage (\n",
        "    lineage_id STRING PRIMARY KEY,\n",
        "    source_object STRING,\n",
        "    target_object STRING,\n",
        "    transformation_type STRING,\n",
        "    business_rule STRING,\n",
        "    created_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n",
        ");\n",
        "\"\"\"\n",
        "execute_sql(conn, create_glossary_sql, \"Business glossary tables created\")\n",
        "\n",
        "# 2. Insert Business Terms\n",
        "print(\"\\nAdding Business Terms...\")\n",
        "insert_terms_sql = \"\"\"\n",
        "INSERT INTO business_glossary.terms VALUES\n",
        "('TERM001', 'Customer Lifetime Value', 'Total revenue generated by a customer over their entire relationship with the company', 'Customer Analytics', 'data_analyst', 'marketing_manager', CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP(), 'ACTIVE'),\n",
        "('TERM002', 'Churn Rate', 'Percentage of customers who stop using the service within a given time period', 'Customer Analytics', 'data_scientist', 'customer_success_manager', CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP(), 'ACTIVE'),\n",
        "('TERM003', 'Monthly Recurring Revenue', 'Total predictable revenue generated from subscriptions each month', 'Financial Metrics', 'finance_analyst', 'cfo', CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP(), 'ACTIVE');\n",
        "\"\"\"\n",
        "execute_sql(conn, insert_terms_sql, \"Business terms added\")\n",
        "\n",
        "# 3. Data Lineage Documentation\n",
        "print(\"\\nDocumenting Data Lineage...\")\n",
        "lineage_sql = \"\"\"\n",
        "INSERT INTO business_glossary.data_lineage VALUES\n",
        "('LINEAGE001', 'customers', 'customer_kpi', 'AGGREGATION', 'Calculate customer metrics from raw customer data', CURRENT_TIMESTAMP()),\n",
        "('LINEAGE002', 'sales_data', 'revenue_analytics', 'AGGREGATION', 'Aggregate sales data by month for revenue analysis', CURRENT_TIMESTAMP());\n",
        "\"\"\"\n",
        "execute_sql(conn, lineage_sql, \"Data lineage documented\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 10. Workflows\n",
        "\n",
        "Workflows in Horizon Catalog enable automated data pipelines and business processes, providing orchestration capabilities for complex data operations.\n",
        "\n",
        "## Workflow Features:\n",
        "- **Task Orchestration**: Coordinate multiple data operations\n",
        "- **Dependency Management**: Handle task dependencies\n",
        "- **Error Handling**: Robust error management and retry logic\n",
        "- **Scheduling**: Automated execution schedules\n",
        "- **Monitoring**: Real-time workflow monitoring\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Workflow Examples\n",
        "\n",
        "# 1. Create Workflow Tasks\n",
        "print(\"Creating Workflow Tasks...\")\n",
        "create_tasks_sql = \"\"\"\n",
        "CREATE OR REPLACE TASK data_ingestion_task\n",
        "WAREHOUSE = 'COMPUTE_WH'\n",
        "SCHEDULE = 'USING CRON 0 2 * * * UTC'  -- Daily at 2 AM UTC\n",
        "AS\n",
        "INSERT INTO analytics_db.staging.daily_sales_summary\n",
        "SELECT \n",
        "    DATE(order_date) as sale_date,\n",
        "    COUNT(*) as total_orders,\n",
        "    SUM(total_amount) as total_revenue,\n",
        "    COUNT(DISTINCT customer_id) as unique_customers\n",
        "FROM iceberg_catalog.sales_data\n",
        "WHERE DATE(order_date) = CURRENT_DATE() - 1\n",
        "GROUP BY DATE(order_date);\n",
        "\n",
        "CREATE OR REPLACE TASK data_quality_check_task\n",
        "WAREHOUSE = 'COMPUTE_WH'\n",
        "AFTER data_ingestion_task\n",
        "AS\n",
        "INSERT INTO analytics_db.staging.data_quality_log\n",
        "SELECT \n",
        "    'daily_sales_summary' as table_name,\n",
        "    COUNT(*) as record_count,\n",
        "    SUM(CASE WHEN total_revenue < 0 THEN 1 ELSE 0 END) as negative_revenue_count,\n",
        "    CURRENT_TIMESTAMP() as check_time\n",
        "FROM analytics_db.staging.daily_sales_summary\n",
        "WHERE sale_date = CURRENT_DATE() - 1;\n",
        "\"\"\"\n",
        "execute_sql(conn, create_tasks_sql, \"Workflow tasks created\")\n",
        "\n",
        "# 2. Enable Tasks\n",
        "print(\"\\nEnabling Workflow Tasks...\")\n",
        "enable_tasks_sql = \"\"\"\n",
        "ALTER TASK data_ingestion_task RESUME;\n",
        "ALTER TASK data_quality_check_task RESUME;\n",
        "\"\"\"\n",
        "execute_sql(conn, enable_tasks_sql, \"Tasks enabled\")\n",
        "\n",
        "# 3. Workflow Monitoring\n",
        "print(\"\\nWorkflow Monitoring...\")\n",
        "workflow_monitoring_sql = \"\"\"\n",
        "SELECT \n",
        "    task_name,\n",
        "    state,\n",
        "    schedule,\n",
        "    next_scheduled_time,\n",
        "    created_on\n",
        "FROM snowflake.information_schema.tasks\n",
        "WHERE task_schema = 'PUBLIC'\n",
        "ORDER BY created_on DESC;\n",
        "\"\"\"\n",
        "df_workflow_monitoring = execute_sql(conn, workflow_monitoring_sql, \"Workflow monitoring completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 11. RBAC (Role-Based Access Control)\n",
        "\n",
        "Role-Based Access Control provides fine-grained security controls for data access, ensuring users only access data they're authorized to see.\n",
        "\n",
        "## RBAC Features:\n",
        "- **Role Hierarchy**: Structured permission inheritance\n",
        "- **Object-Level Security**: Granular access controls\n",
        "- **Dynamic Data Masking**: Protect sensitive data\n",
        "- **Row-Level Security**: Filter data by user context\n",
        "- **Column-Level Security**: Control access to specific columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RBAC Examples\n",
        "\n",
        "# 1. Create Security Roles\n",
        "print(\"Creating Security Roles...\")\n",
        "create_roles_sql = \"\"\"\n",
        "CREATE OR REPLACE ROLE data_analyst;\n",
        "CREATE OR REPLACE ROLE data_scientist;\n",
        "CREATE OR REPLACE ROLE business_user;\n",
        "CREATE OR REPLACE ROLE data_admin;\n",
        "\"\"\"\n",
        "execute_sql(conn, create_roles_sql, \"Security roles created\")\n",
        "\n",
        "# 2. Grant Permissions\n",
        "print(\"\\nGranting Permissions...\")\n",
        "grant_permissions_sql = \"\"\"\n",
        "-- Data Analyst permissions\n",
        "GRANT USAGE ON DATABASE HORIZON_CATALOG_DEMO TO ROLE data_analyst;\n",
        "GRANT USAGE ON SCHEMA HORIZON_CATALOG_DEMO.PUBLIC TO ROLE data_analyst;\n",
        "GRANT SELECT ON TABLE HORIZON_CATALOG_DEMO.PUBLIC.customers TO ROLE data_analyst;\n",
        "GRANT SELECT ON VIEW HORIZON_CATALOG_DEMO.PUBLIC.customer_summary TO ROLE data_analyst;\n",
        "\n",
        "-- Data Scientist permissions\n",
        "GRANT USAGE ON DATABASE HORIZON_CATALOG_DEMO TO ROLE data_scientist;\n",
        "GRANT USAGE ON SCHEMA HORIZON_CATALOG_DEMO.PUBLIC TO ROLE data_scientist;\n",
        "GRANT SELECT ON ALL TABLES IN SCHEMA HORIZON_CATALOG_DEMO.PUBLIC TO ROLE data_scientist;\n",
        "GRANT USAGE ON SCHEMA HORIZON_CATALOG_DEMO.ml_models TO ROLE data_scientist;\n",
        "\n",
        "-- Business User permissions\n",
        "GRANT USAGE ON DATABASE HORIZON_CATALOG_DEMO TO ROLE business_user;\n",
        "GRANT USAGE ON SCHEMA HORIZON_CATALOG_DEMO.PUBLIC TO ROLE business_user;\n",
        "GRANT SELECT ON VIEW HORIZON_CATALOG_DEMO.PUBLIC.customer_summary TO ROLE business_user;\n",
        "\"\"\"\n",
        "execute_sql(conn, grant_permissions_sql, \"Permissions granted\")\n",
        "\n",
        "# 3. Dynamic Data Masking\n",
        "print(\"\\nSetting up Dynamic Data Masking...\")\n",
        "data_masking_sql = \"\"\"\n",
        "CREATE OR REPLACE MASKING POLICY email_mask AS (val STRING) RETURNS STRING ->\n",
        "    CASE \n",
        "        WHEN CURRENT_ROLE() IN ('DATA_ADMIN') THEN val\n",
        "        ELSE REGEXP_REPLACE(val, '(.*)@(.*)', '***@\\\\2')\n",
        "    END;\n",
        "\n",
        "ALTER TABLE customers MODIFY COLUMN email SET MASKING POLICY email_mask;\n",
        "\"\"\"\n",
        "execute_sql(conn, data_masking_sql, \"Dynamic data masking configured\")\n",
        "\n",
        "# 4. Row-Level Security\n",
        "print(\"\\nSetting up Row-Level Security...\")\n",
        "row_level_security_sql = \"\"\"\n",
        "CREATE OR REPLACE ROW ACCESS POLICY customer_region_policy AS (region STRING) RETURNS BOOLEAN ->\n",
        "    CASE \n",
        "        WHEN CURRENT_ROLE() = 'DATA_ADMIN' THEN TRUE\n",
        "        WHEN CURRENT_ROLE() = 'DATA_ANALYST' AND region = 'US' THEN TRUE\n",
        "        ELSE FALSE\n",
        "    END;\n",
        "\n",
        "ALTER TABLE customers ADD ROW ACCESS POLICY customer_region_policy ON (state);\n",
        "\"\"\"\n",
        "execute_sql(conn, row_level_security_sql, \"Row-level security configured\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 12. Audit Trail & Logging\n",
        "\n",
        "Comprehensive audit trails and logging provide visibility into data access, changes, and system activities for compliance and security monitoring.\n",
        "\n",
        "## Audit Features:\n",
        "- **Access Logging**: Track who accessed what data\n",
        "- **Change Tracking**: Monitor data modifications\n",
        "- **Query History**: Log all SQL queries and performance\n",
        "- **Security Events**: Monitor authentication and authorization\n",
        "- **Compliance Reporting**: Generate audit reports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Audit Trail & Logging Examples\n",
        "\n",
        "# 1. Query History Analysis\n",
        "print(\"Analyzing Query History...\")\n",
        "query_history_sql = \"\"\"\n",
        "SELECT \n",
        "    query_id,\n",
        "    user_name,\n",
        "    role_name,\n",
        "    query_text,\n",
        "    start_time,\n",
        "    end_time,\n",
        "    total_elapsed_time,\n",
        "    bytes_scanned,\n",
        "    rows_produced\n",
        "FROM snowflake.account_usage.query_history\n",
        "WHERE start_time >= DATEADD(day, -7, CURRENT_DATE())\n",
        "ORDER BY start_time DESC\n",
        "LIMIT 10;\n",
        "\"\"\"\n",
        "df_query_history = execute_sql(conn, query_history_sql, \"Query history analyzed\")\n",
        "\n",
        "# 2. Access History Analysis\n",
        "print(\"\\nAnalyzing Access History...\")\n",
        "access_history_sql = \"\"\"\n",
        "SELECT \n",
        "    user_name,\n",
        "    role_name,\n",
        "    object_name,\n",
        "    object_domain,\n",
        "    access_type,\n",
        "    access_time\n",
        "FROM snowflake.account_usage.access_history\n",
        "WHERE access_time >= DATEADD(day, -7, CURRENT_DATE())\n",
        "ORDER BY access_time DESC\n",
        "LIMIT 10;\n",
        "\"\"\"\n",
        "df_access_history = execute_sql(conn, access_history_sql, \"Access history analyzed\")\n",
        "\n",
        "# 3. Login History Analysis\n",
        "print(\"\\nAnalyzing Login History...\")\n",
        "login_history_sql = \"\"\"\n",
        "SELECT \n",
        "    user_name,\n",
        "    client_ip,\n",
        "    reported_client_type,\n",
        "    first_authentication_factor,\n",
        "    second_authentication_factor,\n",
        "    login_timestamp,\n",
        "    is_success\n",
        "FROM snowflake.account_usage.login_history\n",
        "WHERE login_timestamp >= DATEADD(day, -7, CURRENT_DATE())\n",
        "ORDER BY login_timestamp DESC\n",
        "LIMIT 10;\n",
        "\"\"\"\n",
        "df_login_history = execute_sql(conn, login_history_sql, \"Login history analyzed\")\n",
        "\n",
        "# 4. Data Change Tracking\n",
        "print(\"\\nData Change Tracking...\")\n",
        "change_tracking_sql = \"\"\"\n",
        "SELECT \n",
        "    table_name,\n",
        "    column_name,\n",
        "    data_type,\n",
        "    is_nullable,\n",
        "    created,\n",
        "    last_altered\n",
        "FROM snowflake.information_schema.columns\n",
        "WHERE table_schema = 'PUBLIC'\n",
        "  AND last_altered >= DATEADD(day, -7, CURRENT_DATE())\n",
        "ORDER BY last_altered DESC;\n",
        "\"\"\"\n",
        "df_change_tracking = execute_sql(conn, change_tracking_sql, \"Data changes tracked\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 13. Metadata Retrieval\n",
        "\n",
        "Metadata retrieval provides comprehensive information about data objects, their relationships, and usage patterns for data discovery and governance.\n",
        "\n",
        "## Metadata Features:\n",
        "- **Object Discovery**: Find and catalog data objects\n",
        "- **Relationship Mapping**: Understand data dependencies\n",
        "- **Usage Analytics**: Track object utilization\n",
        "- **Schema Evolution**: Monitor structural changes\n",
        "- **Data Profiling**: Analyze data characteristics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Metadata Retrieval Examples\n",
        "\n",
        "# 1. Object Discovery\n",
        "print(\"Discovering Data Objects...\")\n",
        "object_discovery_sql = \"\"\"\n",
        "SELECT \n",
        "    table_catalog as database_name,\n",
        "    table_schema as schema_name,\n",
        "    table_name,\n",
        "    table_type,\n",
        "    row_count,\n",
        "    ROUND(bytes / 1024 / 1024, 2) as size_mb,\n",
        "    created,\n",
        "    last_altered\n",
        "FROM snowflake.information_schema.tables\n",
        "WHERE table_schema NOT IN ('INFORMATION_SCHEMA')\n",
        "ORDER BY bytes DESC;\n",
        "\"\"\"\n",
        "df_object_discovery = execute_sql(conn, object_discovery_sql, \"Objects discovered\")\n",
        "\n",
        "# 2. Column Metadata Analysis\n",
        "print(\"\\nColumn Metadata Analysis...\")\n",
        "column_metadata_sql = \"\"\"\n",
        "SELECT \n",
        "    table_name,\n",
        "    column_name,\n",
        "    data_type,\n",
        "    is_nullable,\n",
        "    column_default,\n",
        "    ordinal_position\n",
        "FROM snowflake.information_schema.columns\n",
        "WHERE table_schema = 'PUBLIC'\n",
        "ORDER BY table_name, ordinal_position;\n",
        "\"\"\"\n",
        "df_column_metadata = execute_sql(conn, column_metadata_sql, \"Column metadata analyzed\")\n",
        "\n",
        "# 3. Data Dependencies\n",
        "print(\"\\nData Dependencies...\")\n",
        "dependencies_sql = \"\"\"\n",
        "SELECT \n",
        "    referencing_object_name,\n",
        "    referencing_object_domain,\n",
        "    referenced_object_name,\n",
        "    referenced_object_domain,\n",
        "    referenced_object_id\n",
        "FROM snowflake.account_usage.object_dependencies\n",
        "WHERE referencing_object_domain = 'TABLE'\n",
        "ORDER BY referencing_object_name;\n",
        "\"\"\"\n",
        "df_dependencies = execute_sql(conn, dependencies_sql, \"Dependencies analyzed\")\n",
        "\n",
        "# 4. Data Profiling\n",
        "print(\"\\nData Profiling...\")\n",
        "data_profiling_sql = \"\"\"\n",
        "SELECT \n",
        "    'customers' as table_name,\n",
        "    COUNT(*) as total_rows,\n",
        "    COUNT(DISTINCT customer_id) as unique_customers,\n",
        "    COUNT(DISTINCT city) as unique_cities,\n",
        "    COUNT(DISTINCT state) as unique_states,\n",
        "    MIN(created_date) as earliest_record,\n",
        "    MAX(created_date) as latest_record\n",
        "FROM customers;\n",
        "\"\"\"\n",
        "df_data_profiling = execute_sql(conn, data_profiling_sql, \"Data profiling completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 14. Data Quality\n",
        "\n",
        "Data quality management ensures data accuracy, completeness, and consistency across the organization through automated validation and monitoring.\n",
        "\n",
        "## Data Quality Features:\n",
        "- **Validation Rules**: Define data quality constraints\n",
        "- **Automated Monitoring**: Continuous quality checks\n",
        "- **Data Profiling**: Analyze data characteristics\n",
        "- **Anomaly Detection**: Identify data issues\n",
        "- **Quality Metrics**: Track data quality trends\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Quality Examples\n",
        "\n",
        "# 1. Create Data Quality Schema\n",
        "print(\"Setting up Data Quality Framework...\")\n",
        "create_dq_schema_sql = \"\"\"\n",
        "CREATE OR REPLACE SCHEMA data_quality;\n",
        "\n",
        "CREATE OR REPLACE TABLE data_quality.validation_rules (\n",
        "    rule_id STRING PRIMARY KEY,\n",
        "    table_name STRING,\n",
        "    column_name STRING,\n",
        "    rule_type STRING,\n",
        "    rule_expression STRING,\n",
        "    severity STRING,\n",
        "    created_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n",
        ");\n",
        "\n",
        "CREATE OR REPLACE TABLE data_quality.quality_results (\n",
        "    check_id STRING PRIMARY KEY,\n",
        "    table_name STRING,\n",
        "    rule_id STRING,\n",
        "    check_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),\n",
        "    passed_count INTEGER,\n",
        "    failed_count INTEGER,\n",
        "    total_count INTEGER,\n",
        "    quality_score FLOAT\n",
        ");\n",
        "\"\"\"\n",
        "execute_sql(conn, create_dq_schema_sql, \"Data quality framework created\")\n",
        "\n",
        "# 2. Define Data Quality Rules\n",
        "print(\"\\nDefining Data Quality Rules...\")\n",
        "insert_rules_sql = \"\"\"\n",
        "INSERT INTO data_quality.validation_rules VALUES\n",
        "('RULE001', 'customers', 'customer_id', 'NOT_NULL', 'customer_id IS NOT NULL', 'ERROR'),\n",
        "('RULE002', 'customers', 'email', 'EMAIL_FORMAT', 'email LIKE ''%@%.%''', 'WARNING'),\n",
        "('RULE003', 'customers', 'phone', 'PHONE_FORMAT', 'phone LIKE ''555-%''', 'WARNING'),\n",
        "('RULE004', 'iceberg_catalog.sales_data', 'total_amount', 'POSITIVE_VALUE', 'total_amount > 0', 'ERROR'),\n",
        "('RULE005', 'iceberg_catalog.sales_data', 'quantity', 'POSITIVE_VALUE', 'quantity > 0', 'ERROR');\n",
        "\"\"\"\n",
        "execute_sql(conn, insert_rules_sql, \"Data quality rules defined\")\n",
        "\n",
        "# 3. Data Quality Checks\n",
        "print(\"\\nRunning Data Quality Checks...\")\n",
        "quality_check_sql = \"\"\"\n",
        "INSERT INTO data_quality.quality_results\n",
        "SELECT \n",
        "    'CHECK_' || CURRENT_TIMESTAMP() as check_id,\n",
        "    'customers' as table_name,\n",
        "    'RULE001' as rule_id,\n",
        "    CURRENT_TIMESTAMP() as check_date,\n",
        "    SUM(CASE WHEN customer_id IS NOT NULL THEN 1 ELSE 0 END) as passed_count,\n",
        "    SUM(CASE WHEN customer_id IS NULL THEN 1 ELSE 0 END) as failed_count,\n",
        "    COUNT(*) as total_count,\n",
        "    ROUND(SUM(CASE WHEN customer_id IS NOT NULL THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as quality_score\n",
        "FROM customers;\n",
        "\"\"\"\n",
        "execute_sql(conn, quality_check_sql, \"Data quality check completed\")\n",
        "\n",
        "# 4. Data Quality Dashboard\n",
        "print(\"\\nData Quality Dashboard...\")\n",
        "quality_dashboard_sql = \"\"\"\n",
        "SELECT \n",
        "    qr.table_name,\n",
        "    vr.rule_type,\n",
        "    qr.passed_count,\n",
        "    qr.failed_count,\n",
        "    qr.total_count,\n",
        "    qr.quality_score,\n",
        "    qr.check_date\n",
        "FROM data_quality.quality_results qr\n",
        "JOIN data_quality.validation_rules vr ON qr.rule_id = vr.rule_id\n",
        "ORDER BY qr.check_date DESC;\n",
        "\"\"\"\n",
        "df_quality_dashboard = execute_sql(conn, quality_dashboard_sql, \"Quality dashboard generated\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 15. Practical Examples & Best Practices\n",
        "\n",
        "This section provides real-world examples and best practices for implementing Horizon Catalog features effectively in production environments.\n",
        "\n",
        "## Best Practices:\n",
        "- **Data Architecture**: Design scalable data architectures\n",
        "- **Performance Optimization**: Optimize queries and storage\n",
        "- **Security Implementation**: Implement comprehensive security\n",
        "- **Monitoring & Alerting**: Set up effective monitoring\n",
        "- **Cost Management**: Optimize costs and resource usage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Practical Examples & Best Practices\n",
        "\n",
        "# 1. Complete Data Architecture Example\n",
        "print(\"Complete Data Architecture Implementation...\")\n",
        "\n",
        "# Create comprehensive data architecture\n",
        "architecture_sql = \"\"\"\n",
        "-- Raw Data Layer\n",
        "CREATE OR REPLACE SCHEMA raw_data\n",
        "COMMENT = 'Raw data ingestion layer';\n",
        "\n",
        "-- Staging Layer  \n",
        "CREATE OR REPLACE SCHEMA staging\n",
        "COMMENT = 'Data staging and transformation layer';\n",
        "\n",
        "-- Data Mart Layer\n",
        "CREATE OR REPLACE SCHEMA data_mart\n",
        "COMMENT = 'Business-ready data marts';\n",
        "\n",
        "-- ML Layer\n",
        "CREATE OR REPLACE SCHEMA ml_models\n",
        "COMMENT = 'Machine learning models and functions';\n",
        "\n",
        "-- Analytics Layer\n",
        "CREATE OR REPLACE SCHEMA analytics\n",
        "COMMENT = 'Analytics and reporting layer';\n",
        "\"\"\"\n",
        "execute_sql(conn, architecture_sql, \"Data architecture created\")\n",
        "\n",
        "# 2. Performance Optimization Best Practices\n",
        "print(\"\\nPerformance Optimization Examples...\")\n",
        "performance_optimization_sql = \"\"\"\n",
        "-- Create optimized table with clustering\n",
        "CREATE OR REPLACE TABLE analytics.customer_performance (\n",
        "    customer_id STRING,\n",
        "    order_date DATE,\n",
        "    total_amount DECIMAL(12,2),\n",
        "    order_count INTEGER\n",
        ")\n",
        "CLUSTER BY (customer_id, order_date);\n",
        "\n",
        "-- Create materialized view for frequently accessed data\n",
        "CREATE OR REPLACE MATERIALIZED VIEW analytics.daily_sales_summary\n",
        "AS\n",
        "SELECT \n",
        "    DATE(order_date) as sale_date,\n",
        "    COUNT(*) as total_orders,\n",
        "    SUM(total_amount) as total_revenue,\n",
        "    COUNT(DISTINCT customer_id) as unique_customers\n",
        "FROM iceberg_catalog.sales_data\n",
        "GROUP BY DATE(order_date);\n",
        "\"\"\"\n",
        "execute_sql(conn, performance_optimization_sql, \"Performance optimizations applied\")\n",
        "\n",
        "# 3. Comprehensive Security Implementation\n",
        "print(\"\\nComprehensive Security Implementation...\")\n",
        "security_implementation_sql = \"\"\"\n",
        "-- Create comprehensive role hierarchy\n",
        "CREATE OR REPLACE ROLE data_engineer;\n",
        "CREATE OR REPLACE ROLE data_analyst;\n",
        "CREATE OR REPLACE ROLE business_user;\n",
        "CREATE OR REPLACE ROLE data_admin;\n",
        "\n",
        "-- Grant appropriate permissions\n",
        "GRANT USAGE ON DATABASE HORIZON_CATALOG_DEMO TO ROLE data_engineer;\n",
        "GRANT ALL PRIVILEGES ON SCHEMA HORIZON_CATALOG_DEMO.raw_data TO ROLE data_engineer;\n",
        "GRANT ALL PRIVILEGES ON SCHEMA HORIZON_CATALOG_DEMO.staging TO ROLE data_engineer;\n",
        "\n",
        "GRANT USAGE ON DATABASE HORIZON_CATALOG_DEMO TO ROLE data_analyst;\n",
        "GRANT SELECT ON ALL TABLES IN SCHEMA HORIZON_CATALOG_DEMO.staging TO ROLE data_analyst;\n",
        "GRANT SELECT ON ALL TABLES IN SCHEMA HORIZON_CATALOG_DEMO.data_mart TO ROLE data_analyst;\n",
        "\n",
        "GRANT USAGE ON DATABASE HORIZON_CATALOG_DEMO TO ROLE business_user;\n",
        "GRANT SELECT ON ALL TABLES IN SCHEMA HORIZON_CATALOG_DEMO.data_mart TO ROLE business_user;\n",
        "\"\"\"\n",
        "execute_sql(conn, security_implementation_sql, \"Security implementation completed\")\n",
        "\n",
        "# 4. Monitoring and Alerting Setup\n",
        "print(\"\\nMonitoring and Alerting Setup...\")\n",
        "monitoring_setup_sql = \"\"\"\n",
        "-- Create monitoring dashboard\n",
        "CREATE OR REPLACE VIEW monitoring.system_health AS\n",
        "SELECT \n",
        "    'Warehouse Usage' as metric_name,\n",
        "    warehouse_name,\n",
        "    SUM(credits_used) as total_credits,\n",
        "    COUNT(*) as query_count\n",
        "FROM snowflake.account_usage.query_history\n",
        "WHERE start_time >= DATEADD(day, -1, CURRENT_DATE())\n",
        "GROUP BY warehouse_name\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT \n",
        "    'Storage Usage' as metric_name,\n",
        "    database_name,\n",
        "    SUM(bytes) as total_bytes,\n",
        "    COUNT(*) as table_count\n",
        "FROM snowflake.information_schema.tables\n",
        "GROUP BY database_name;\n",
        "\"\"\"\n",
        "execute_sql(conn, monitoring_setup_sql, \"Monitoring setup completed\")\n",
        "\n",
        "# 5. Cost Optimization Recommendations\n",
        "print(\"\\nCost Optimization Analysis...\")\n",
        "cost_optimization_sql = \"\"\"\n",
        "-- Identify cost optimization opportunities\n",
        "SELECT \n",
        "    'High Credit Queries' as optimization_type,\n",
        "    COUNT(*) as count,\n",
        "    SUM(credits_used) as total_credits\n",
        "FROM snowflake.account_usage.query_history\n",
        "WHERE credits_used > 1.0\n",
        "  AND start_time >= DATEADD(day, -7, CURRENT_DATE())\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT \n",
        "    'Large Tables' as optimization_type,\n",
        "    COUNT(*) as count,\n",
        "    SUM(bytes) as total_bytes\n",
        "FROM snowflake.information_schema.tables\n",
        "WHERE bytes > 1024 * 1024 * 1024;  -- Tables larger than 1GB\n",
        "\"\"\"\n",
        "df_cost_optimization = execute_sql(conn, cost_optimization_sql, \"Cost optimization analysis completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusion & Summary\n",
        "\n",
        "This comprehensive notebook has demonstrated all major features of Snowflake's Horizon Catalog, providing practical examples and best practices for implementation.\n",
        "\n",
        "## Key Takeaways:\n",
        "\n",
        "### 1. **Iceberg Tables**\n",
        "- Provide ACID transactions and schema evolution\n",
        "- Enable better interoperability with open data formats\n",
        "- Support time travel and partition evolution\n",
        "\n",
        "### 2. **Traditional Tables & Views**\n",
        "- Foundation for data storage and access patterns\n",
        "- Support for various table types (permanent, temporary, transient, external)\n",
        "- Materialized views for performance optimization\n",
        "\n",
        "### 3. **AI & ML Objects**\n",
        "- Centralized model registry and management\n",
        "- ML functions for business logic\n",
        "- Vector embeddings for semantic search\n",
        "\n",
        "### 4. **Cost & Usage Monitoring**\n",
        "- Comprehensive cost tracking and optimization\n",
        "- Resource usage analytics\n",
        "- Budget alerts and monitoring\n",
        "\n",
        "### 5. **Data Warehouses**\n",
        "- Multi-cluster auto-scaling capabilities\n",
        "- Resource monitors for cost control\n",
        "- Performance optimization features\n",
        "\n",
        "## Next Steps:\n",
        "\n",
        "1. **Implement Gradually**: Start with core features and expand over time\n",
        "2. **Establish Governance**: Set up data governance policies and procedures\n",
        "3. **Monitor Performance**: Continuously monitor and optimize performance\n",
        "4. **Train Teams**: Ensure teams understand the new capabilities\n",
        "5. **Iterate and Improve**: Regularly review and improve implementations\n",
        "\n",
        "## Resources:\n",
        "\n",
        "- [Snowflake Documentation](https://docs.snowflake.com/)\n",
        "- [Horizon Catalog Guide](https://docs.snowflake.com/en/user-guide/horizon-catalog)\n",
        "- [Best Practices Guide](https://docs.snowflake.com/en/user-guide/best-practices)\n",
        "- [Security Guide](https://docs.snowflake.com/en/user-guide/security)\n",
        "\n",
        "---\n",
        "\n",
        "**Note**: Remember to update the connection configuration at the beginning of this notebook with your actual Snowflake account details before running the examples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 16. Data Metric Functions (DMFs) - Advanced Data Quality\n",
        "\n",
        "Data Metric Functions (DMFs) are Snowflake's advanced data quality monitoring system that provides automated measurement and monitoring of data quality metrics. Based on the Snowflake Data Quality documentation, DMFs enable comprehensive data quality management.\n",
        "\n",
        "## DMF Features:\n",
        "- **System DMFs**: Built-in metrics from Snowflake\n",
        "- **Custom DMFs**: User-defined quality metrics\n",
        "- **Automated Scheduling**: Regular quality checks\n",
        "- **Expectations**: Define pass/fail criteria\n",
        "- **Quality Monitoring**: Track quality trends over time\n",
        "- **Alerting**: Notify on quality issues\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Metric Functions (DMFs) Examples\n",
        "\n",
        "# 1. System DMFs - Built-in Quality Metrics\n",
        "print(\"Using System DMFs for Data Quality...\")\n",
        "\n",
        "# Create a table to demonstrate DMFs\n",
        "create_demo_table_sql = \"\"\"\n",
        "CREATE OR REPLACE TABLE dq_demo.customer_data (\n",
        "    customer_id STRING,\n",
        "    first_name STRING,\n",
        "    last_name STRING,\n",
        "    email STRING,\n",
        "    phone STRING,\n",
        "    age INTEGER,\n",
        "    registration_date DATE,\n",
        "    status STRING\n",
        ");\n",
        "\"\"\"\n",
        "execute_sql(conn, create_demo_table_sql, \"Demo table created for DMFs\")\n",
        "\n",
        "# Insert sample data with some quality issues\n",
        "insert_demo_data_sql = \"\"\"\n",
        "INSERT INTO dq_demo.customer_data VALUES\n",
        "('CUST001', 'John', 'Doe', 'john.doe@email.com', '555-0101', 25, '2024-01-15', 'ACTIVE'),\n",
        "('CUST002', 'Jane', 'Smith', 'jane.smith@email.com', '555-0102', 30, '2024-01-16', 'ACTIVE'),\n",
        "('CUST003', 'Bob', 'Johnson', 'bob.johnson@email.com', '555-0103', 35, '2024-01-17', 'ACTIVE'),\n",
        "('CUST004', 'Alice', 'Brown', 'alice.brown@email.com', '555-0104', 28, '2024-01-18', 'ACTIVE'),\n",
        "('CUST005', 'Charlie', 'Wilson', 'charlie.wilson@email.com', '555-0105', 42, '2024-01-19', 'ACTIVE'),\n",
        "('CUST006', 'Diana', 'Davis', 'diana.davis@email.com', '555-0106', 29, '2024-01-20', 'ACTIVE'),\n",
        "('CUST007', 'Eve', 'Miller', 'eve.miller@email.com', '555-0107', 31, '2024-01-21', 'ACTIVE'),\n",
        "('CUST008', 'Frank', 'Garcia', 'frank.garcia@email.com', '555-0108', 27, '2024-01-22', 'ACTIVE'),\n",
        "('CUST009', 'Grace', 'Martinez', 'grace.martinez@email.com', '555-0109', 33, '2024-01-23', 'ACTIVE'),\n",
        "('CUST010', 'Henry', 'Anderson', 'henry.anderson@email.com', '555-0110', 26, '2024-01-24', 'ACTIVE');\n",
        "\"\"\"\n",
        "execute_sql(conn, insert_demo_data_sql, \"Sample data inserted\")\n",
        "\n",
        "# 2. Using System DMFs\n",
        "print(\"\\nUsing System DMFs...\")\n",
        "\n",
        "# Row count DMF\n",
        "row_count_dmf_sql = \"\"\"\n",
        "SELECT SNOWFLAKE.CORE.ROW_COUNT('dq_demo.customer_data') as total_rows;\n",
        "\"\"\"\n",
        "df_row_count = execute_sql(conn, row_count_dmf_sql, \"Row count DMF executed\")\n",
        "\n",
        "# Null count DMF\n",
        "null_count_dmf_sql = \"\"\"\n",
        "SELECT SNOWFLAKE.CORE.NULL_COUNT('dq_demo.customer_data', 'email') as null_email_count;\n",
        "\"\"\"\n",
        "df_null_count = execute_sql(conn, null_count_dmf_sql, \"Null count DMF executed\")\n",
        "\n",
        "# Unique count DMF\n",
        "unique_count_dmf_sql = \"\"\"\n",
        "SELECT SNOWFLAKE.CORE.UNIQUE_COUNT('dq_demo.customer_data', 'customer_id') as unique_customers;\n",
        "\"\"\"\n",
        "df_unique_count = execute_sql(conn, unique_count_dmf_sql, \"Unique count DMF executed\")\n",
        "\n",
        "# Duplicate count DMF\n",
        "duplicate_count_dmf_sql = \"\"\"\n",
        "SELECT SNOWFLAKE.CORE.DUPLICATE_COUNT('dq_demo.customer_data', 'email') as duplicate_emails;\n",
        "\"\"\"\n",
        "df_duplicate_count = execute_sql(conn, duplicate_count_dmf_sql, \"Duplicate count DMF executed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom DMFs and Advanced Quality Monitoring\n",
        "\n",
        "# 3. Create Custom DMFs\n",
        "print(\"\\nCreating Custom DMFs...\")\n",
        "\n",
        "# Custom DMF for email format validation\n",
        "create_email_dmf_sql = \"\"\"\n",
        "CREATE OR REPLACE DATA METRIC FUNCTION dq_demo.email_format_check(TABLE_NAME STRING, COLUMN_NAME STRING)\n",
        "RETURNS FLOAT\n",
        "LANGUAGE SQL\n",
        "AS\n",
        "$$\n",
        "    SELECT \n",
        "        COUNT(CASE WHEN REGEXP_LIKE($2, '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Za-z]{2,}$') THEN 1 END) * 100.0 / COUNT(*)\n",
        "    FROM IDENTIFIER($1);\n",
        "$$;\n",
        "\"\"\"\n",
        "execute_sql(conn, create_email_dmf_sql, \"Email format DMF created\")\n",
        "\n",
        "# Custom DMF for age range validation\n",
        "create_age_dmf_sql = \"\"\"\n",
        "CREATE OR REPLACE DATA METRIC FUNCTION dq_demo.age_range_check(TABLE_NAME STRING, COLUMN_NAME STRING)\n",
        "RETURNS FLOAT\n",
        "LANGUAGE SQL\n",
        "AS\n",
        "$$\n",
        "    SELECT \n",
        "        COUNT(CASE WHEN $2 BETWEEN 18 AND 100 THEN 1 END) * 100.0 / COUNT(*)\n",
        "    FROM IDENTIFIER($1);\n",
        "$$;\n",
        "\"\"\"\n",
        "execute_sql(conn, create_age_dmf_sql, \"Age range DMF created\")\n",
        "\n",
        "# Custom DMF for data freshness\n",
        "create_freshness_dmf_sql = \"\"\"\n",
        "CREATE OR REPLACE DATA METRIC FUNCTION dq_demo.data_freshness(TABLE_NAME STRING, COLUMN_NAME STRING)\n",
        "RETURNS FLOAT\n",
        "LANGUAGE SQL\n",
        "AS\n",
        "$$\n",
        "    SELECT \n",
        "        CASE \n",
        "            WHEN COUNT(*) = 0 THEN 0\n",
        "            ELSE COUNT(CASE WHEN $2 >= CURRENT_DATE() - 7 THEN 1 END) * 100.0 / COUNT(*)\n",
        "        END\n",
        "    FROM IDENTIFIER($1);\n",
        "$$;\n",
        "\"\"\"\n",
        "execute_sql(conn, create_freshness_dmf_sql, \"Data freshness DMF created\")\n",
        "\n",
        "# 4. Test Custom DMFs\n",
        "print(\"\\nTesting Custom DMFs...\")\n",
        "\n",
        "# Test email format DMF\n",
        "test_email_dmf_sql = \"\"\"\n",
        "SELECT dq_demo.email_format_check('dq_demo.customer_data', 'email') as email_format_score;\n",
        "\"\"\"\n",
        "df_email_dmf = execute_sql(conn, test_email_dmf_sql, \"Email format DMF tested\")\n",
        "\n",
        "# Test age range DMF\n",
        "test_age_dmf_sql = \"\"\"\n",
        "SELECT dq_demo.age_range_check('dq_demo.customer_data', 'age') as age_range_score;\n",
        "\"\"\"\n",
        "df_age_dmf = execute_sql(conn, test_age_dmf_sql, \"Age range DMF tested\")\n",
        "\n",
        "# Test data freshness DMF\n",
        "test_freshness_dmf_sql = \"\"\"\n",
        "SELECT dq_demo.data_freshness('dq_demo.customer_data', 'registration_date') as data_freshness_score;\n",
        "\"\"\"\n",
        "df_freshness_dmf = execute_sql(conn, test_freshness_dmf_sql, \"Data freshness DMF tested\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DMF Scheduling and Expectations\n",
        "\n",
        "# 5. Set DMFs on Tables with Scheduling\n",
        "print(\"\\nSetting DMFs on Tables with Scheduling...\")\n",
        "\n",
        "# Set system DMFs on the table\n",
        "set_dmf_sql = \"\"\"\n",
        "-- Set row count DMF\n",
        "ALTER TABLE dq_demo.customer_data SET DATA METRIC FUNCTION SNOWFLAKE.CORE.ROW_COUNT;\n",
        "\n",
        "-- Set null count DMF for email column\n",
        "ALTER TABLE dq_demo.customer_data SET DATA METRIC FUNCTION SNOWFLAKE.CORE.NULL_COUNT('email');\n",
        "\n",
        "-- Set unique count DMF for customer_id column\n",
        "ALTER TABLE dq_demo.customer_data SET DATA METRIC FUNCTION SNOWFLAKE.CORE.UNIQUE_COUNT('customer_id');\n",
        "\n",
        "-- Set custom DMFs\n",
        "ALTER TABLE dq_demo.customer_data SET DATA METRIC FUNCTION dq_demo.email_format_check('dq_demo.customer_data', 'email');\n",
        "ALTER TABLE dq_demo.customer_data SET DATA METRIC FUNCTION dq_demo.age_range_check('dq_demo.customer_data', 'age');\n",
        "ALTER TABLE dq_demo.customer_data SET DATA METRIC FUNCTION dq_demo.data_freshness('dq_demo.customer_data', 'registration_date');\n",
        "\"\"\"\n",
        "execute_sql(conn, set_dmf_sql, \"DMFs set on table\")\n",
        "\n",
        "# 6. Schedule DMFs to run automatically\n",
        "print(\"\\nScheduling DMFs...\")\n",
        "schedule_dmf_sql = \"\"\"\n",
        "-- Schedule DMFs to run every hour\n",
        "ALTER TABLE dq_demo.customer_data SET DATA METRIC FUNCTION SCHEDULE = 'USING CRON 0 * * * * UTC';\n",
        "\"\"\"\n",
        "execute_sql(conn, schedule_dmf_sql, \"DMFs scheduled\")\n",
        "\n",
        "# 7. Create Expectations for Quality Checks\n",
        "print(\"\\nCreating Expectations for Quality Checks...\")\n",
        "create_expectations_sql = \"\"\"\n",
        "-- Create expectation for row count (should be > 0)\n",
        "CREATE OR REPLACE EXPECTATION dq_demo.row_count_expectation\n",
        "ON dq_demo.customer_data\n",
        "FOR SNOWFLAKE.CORE.ROW_COUNT\n",
        "EXPECT VALUE > 0;\n",
        "\n",
        "-- Create expectation for email format (should be > 90%)\n",
        "CREATE OR REPLACE EXPECTATION dq_demo.email_format_expectation\n",
        "ON dq_demo.customer_data\n",
        "FOR dq_demo.email_format_check('dq_demo.customer_data', 'email')\n",
        "EXPECT VALUE > 90.0;\n",
        "\n",
        "-- Create expectation for age range (should be > 95%)\n",
        "CREATE OR REPLACE EXPECTATION dq_demo.age_range_expectation\n",
        "ON dq_demo.customer_data\n",
        "FOR dq_demo.age_range_check('dq_demo.customer_data', 'age')\n",
        "EXPECT VALUE > 95.0;\n",
        "\"\"\"\n",
        "execute_sql(conn, create_expectations_sql, \"Expectations created\")\n",
        "\n",
        "# 8. Monitor DMF Results\n",
        "print(\"\\nMonitoring DMF Results...\")\n",
        "monitor_dmf_sql = \"\"\"\n",
        "SELECT \n",
        "    table_name,\n",
        "    metric_function_name,\n",
        "    metric_value,\n",
        "    expectation_result,\n",
        "    check_time\n",
        "FROM snowflake.account_usage.data_metric_function_results\n",
        "WHERE table_name = 'CUSTOMER_DATA'\n",
        "ORDER BY check_time DESC\n",
        "LIMIT 10;\n",
        "\"\"\"\n",
        "df_dmf_results = execute_sql(conn, monitor_dmf_sql, \"DMF results monitored\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 17. Advanced Data Governance Features\n",
        "\n",
        "Horizon Catalog provides comprehensive data governance capabilities that go beyond basic data quality monitoring, including advanced classification, lineage tracking, and compliance features.\n",
        "\n",
        "## Advanced Governance Features:\n",
        "- **Sensitive Data Classification**: Automatic detection and tagging of sensitive data\n",
        "- **Data Lineage Tracking**: Complete data flow visualization\n",
        "- **Access Policies**: Advanced row-level and column-level security\n",
        "- **Object Tagging**: Hierarchical tagging system for data organization\n",
        "- **Compliance Reporting**: Automated compliance and audit reporting\n",
        "- **Data Retention Policies**: Automated data lifecycle management\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced Data Governance Examples\n",
        "\n",
        "# 1. Sensitive Data Classification\n",
        "print(\"Setting up Sensitive Data Classification...\")\n",
        "\n",
        "# Create classification tags\n",
        "create_classification_tags_sql = \"\"\"\n",
        "CREATE OR REPLACE TAG governance.data_classification (\n",
        "    sensitivity_level STRING,\n",
        "    data_type STRING,\n",
        "    compliance_standard STRING,\n",
        "    retention_period INTEGER,\n",
        "    data_owner STRING\n",
        ");\n",
        "\n",
        "CREATE OR REPLACE TAG governance.pii_classification (\n",
        "    pii_type STRING,\n",
        "    encryption_required BOOLEAN,\n",
        "    masking_required BOOLEAN,\n",
        "    access_level STRING\n",
        ");\n",
        "\"\"\"\n",
        "execute_sql(conn, create_classification_tags_sql, \"Classification tags created\")\n",
        "\n",
        "# Apply classification tags to tables\n",
        "apply_classification_sql = \"\"\"\n",
        "-- Classify customer data as sensitive\n",
        "ALTER TABLE customers SET TAG governance.data_classification = (\n",
        "    'HIGH',\n",
        "    'CUSTOMER_DATA',\n",
        "    'GDPR',\n",
        "    2555,  -- 7 years\n",
        "    'data_governance_team'\n",
        ");\n",
        "\n",
        "-- Classify email as PII\n",
        "ALTER TABLE customers MODIFY COLUMN email SET TAG governance.pii_classification = (\n",
        "    'EMAIL',\n",
        "    TRUE,\n",
        "    TRUE,\n",
        "    'RESTRICTED'\n",
        ");\n",
        "\n",
        "-- Classify phone as PII\n",
        "ALTER TABLE customers MODIFY COLUMN phone SET TAG governance.pii_classification = (\n",
        "    'PHONE',\n",
        "    TRUE,\n",
        "    TRUE,\n",
        "    'RESTRICTED'\n",
        ");\n",
        "\"\"\"\n",
        "execute_sql(conn, apply_classification_sql, \"Classification tags applied\")\n",
        "\n",
        "# 2. Advanced Access Policies\n",
        "print(\"\\nCreating Advanced Access Policies...\")\n",
        "\n",
        "# Create aggregation policy for sensitive data\n",
        "create_aggregation_policy_sql = \"\"\"\n",
        "CREATE OR REPLACE AGGREGATION POLICY governance.customer_aggregation_policy\n",
        "AS (agg_type STRING) RETURNS AGGREGATION_CONSTRAINT ->\n",
        "    CASE \n",
        "        WHEN agg_type = 'COUNT' THEN AGGREGATION_CONSTRAINT(1, 1000)\n",
        "        WHEN agg_type = 'SUM' THEN AGGREGATION_CONSTRAINT(1, 1000)\n",
        "        WHEN agg_type = 'AVG' THEN AGGREGATION_CONSTRAINT(1, 1000)\n",
        "        ELSE AGGREGATION_CONSTRAINT(1, 1000)\n",
        "    END;\n",
        "\n",
        "-- Apply aggregation policy to sensitive columns\n",
        "ALTER TABLE customers MODIFY COLUMN email SET AGGREGATION POLICY governance.customer_aggregation_policy;\n",
        "ALTER TABLE customers MODIFY COLUMN phone SET AGGREGATION POLICY governance.customer_aggregation_policy;\n",
        "\"\"\"\n",
        "execute_sql(conn, create_aggregation_policy_sql, \"Aggregation policy created\")\n",
        "\n",
        "# 3. Data Lineage Tracking\n",
        "print(\"\\nSetting up Data Lineage Tracking...\")\n",
        "\n",
        "# Create lineage tracking table\n",
        "create_lineage_table_sql = \"\"\"\n",
        "CREATE OR REPLACE TABLE governance.data_lineage (\n",
        "    lineage_id STRING PRIMARY KEY,\n",
        "    source_object STRING,\n",
        "    target_object STRING,\n",
        "    transformation_type STRING,\n",
        "    business_rule STRING,\n",
        "    data_flow_description STRING,\n",
        "    created_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),\n",
        "    updated_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n",
        ");\n",
        "\"\"\"\n",
        "execute_sql(conn, create_lineage_table_sql, \"Lineage tracking table created\")\n",
        "\n",
        "# Insert lineage information\n",
        "insert_lineage_sql = \"\"\"\n",
        "INSERT INTO governance.data_lineage VALUES\n",
        "('LINEAGE001', 'customers', 'customer_summary', 'VIEW', 'Customer data aggregation', 'Creates customer summary view from raw customer data', CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP()),\n",
        "('LINEAGE002', 'iceberg_catalog.sales_data', 'revenue_analytics', 'AGGREGATION', 'Revenue calculation', 'Aggregates sales data by month for revenue analysis', CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP()),\n",
        "('LINEAGE003', 'customers', 'customer_kpi', 'JOIN_AGGREGATION', 'Customer metrics calculation', 'Joins customer data with sales data to calculate KPIs', CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP());\n",
        "\"\"\"\n",
        "execute_sql(conn, insert_lineage_sql, \"Lineage information inserted\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Retention and Compliance Features\n",
        "\n",
        "# 4. Data Retention Policies\n",
        "print(\"\\nSetting up Data Retention Policies...\")\n",
        "\n",
        "# Create retention policy\n",
        "create_retention_policy_sql = \"\"\"\n",
        "CREATE OR REPLACE RETENTION POLICY governance.customer_retention_policy\n",
        "AS (retention_period INTEGER) RETURNS RETENTION_CONSTRAINT ->\n",
        "    CASE \n",
        "        WHEN retention_period <= 365 THEN RETENTION_CONSTRAINT('DELETE', CURRENT_DATE() + retention_period)\n",
        "        WHEN retention_period <= 2555 THEN RETENTION_CONSTRAINT('ARCHIVE', CURRENT_DATE() + retention_period)\n",
        "        ELSE RETENTION_CONSTRAINT('RETAIN', CURRENT_DATE() + retention_period)\n",
        "    END;\n",
        "\n",
        "-- Apply retention policy to customer data\n",
        "ALTER TABLE customers SET RETENTION POLICY governance.customer_retention_policy(2555);  -- 7 years\n",
        "\"\"\"\n",
        "execute_sql(conn, create_retention_policy_sql, \"Retention policy created\")\n",
        "\n",
        "# 5. Compliance Reporting\n",
        "print(\"\\nCreating Compliance Reporting...\")\n",
        "\n",
        "# Create compliance dashboard\n",
        "create_compliance_dashboard_sql = \"\"\"\n",
        "CREATE OR REPLACE VIEW governance.compliance_dashboard AS\n",
        "SELECT \n",
        "    'Data Classification' as compliance_area,\n",
        "    COUNT(*) as total_objects,\n",
        "    COUNT(CASE WHEN sensitivity_level = 'HIGH' THEN 1 END) as high_sensitivity_count,\n",
        "    COUNT(CASE WHEN compliance_standard = 'GDPR' THEN 1 END) as gdpr_compliant_count\n",
        "FROM (\n",
        "    SELECT \n",
        "        table_name,\n",
        "        tag_value:sensitivity_level::STRING as sensitivity_level,\n",
        "        tag_value:compliance_standard::STRING as compliance_standard\n",
        "    FROM snowflake.account_usage.tag_references\n",
        "    WHERE tag_name = 'DATA_CLASSIFICATION'\n",
        ") classified_data\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT \n",
        "    'Data Quality' as compliance_area,\n",
        "    COUNT(*) as total_checks,\n",
        "    COUNT(CASE WHEN expectation_result = 'PASSED' THEN 1 END) as passed_checks,\n",
        "    COUNT(CASE WHEN expectation_result = 'FAILED' THEN 1 END) as failed_checks\n",
        "FROM snowflake.account_usage.data_metric_function_results\n",
        "WHERE check_time >= DATEADD(day, -7, CURRENT_DATE());\n",
        "\"\"\"\n",
        "execute_sql(conn, create_compliance_dashboard_sql, \"Compliance dashboard created\")\n",
        "\n",
        "# 6. Data Privacy Controls\n",
        "print(\"\\nImplementing Data Privacy Controls...\")\n",
        "\n",
        "# Create privacy policy\n",
        "create_privacy_policy_sql = \"\"\"\n",
        "CREATE OR REPLACE PRIVACY POLICY governance.customer_privacy_policy\n",
        "AS (privacy_level STRING) RETURNS PRIVACY_CONSTRAINT ->\n",
        "    CASE \n",
        "        WHEN privacy_level = 'PUBLIC' THEN PRIVACY_CONSTRAINT('ALLOW_ALL')\n",
        "        WHEN privacy_level = 'INTERNAL' THEN PRIVACY_CONSTRAINT('RESTRICT_EXTERNAL')\n",
        "        WHEN privacy_level = 'CONFIDENTIAL' THEN PRIVACY_CONSTRAINT('RESTRICT_ALL')\n",
        "        ELSE PRIVACY_CONSTRAINT('RESTRICT_ALL')\n",
        "    END;\n",
        "\n",
        "-- Apply privacy policy\n",
        "ALTER TABLE customers SET PRIVACY POLICY governance.customer_privacy_policy('CONFIDENTIAL');\n",
        "\"\"\"\n",
        "execute_sql(conn, create_privacy_policy_sql, \"Privacy policy created\")\n",
        "\n",
        "# 7. Governance Monitoring\n",
        "print(\"\\nSetting up Governance Monitoring...\")\n",
        "\n",
        "# Create governance monitoring view\n",
        "governance_monitoring_sql = \"\"\"\n",
        "CREATE OR REPLACE VIEW governance.governance_monitoring AS\n",
        "SELECT \n",
        "    'Tag Usage' as metric_name,\n",
        "    COUNT(*) as total_tags,\n",
        "    COUNT(DISTINCT tag_name) as unique_tags,\n",
        "    COUNT(DISTINCT object_name) as tagged_objects\n",
        "FROM snowflake.account_usage.tag_references\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT \n",
        "    'Access Policies' as metric_name,\n",
        "    COUNT(*) as total_policies,\n",
        "    COUNT(DISTINCT policy_name) as unique_policies,\n",
        "    COUNT(DISTINCT object_name) as protected_objects\n",
        "FROM snowflake.account_usage.policy_references\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT \n",
        "    'Data Quality' as metric_name,\n",
        "    COUNT(*) as total_checks,\n",
        "    COUNT(DISTINCT table_name) as monitored_tables,\n",
        "    COUNT(CASE WHEN expectation_result = 'PASSED' THEN 1 END) as passed_checks\n",
        "FROM snowflake.account_usage.data_metric_function_results\n",
        "WHERE check_time >= DATEADD(day, -7, CURRENT_DATE());\n",
        "\"\"\"\n",
        "execute_sql(conn, governance_monitoring_sql, \"Governance monitoring view created\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 18. Catalog Management and Discovery\n",
        "\n",
        "Horizon Catalog provides advanced catalog management capabilities for data discovery, metadata management, and catalog operations across different data sources and formats.\n",
        "\n",
        "## Catalog Management Features:\n",
        "- **Multi-Catalog Support**: Manage multiple catalogs from different sources\n",
        "- **Catalog Discovery**: Automatic discovery of data sources\n",
        "- **Metadata Synchronization**: Keep metadata in sync across catalogs\n",
        "- **Catalog Federation**: Query across multiple catalogs\n",
        "- **Data Source Integration**: Connect to various data sources\n",
        "- **Catalog Versioning**: Track catalog changes over time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Catalog Management and Discovery Examples\n",
        "\n",
        "# 1. Multi-Catalog Setup\n",
        "print(\"Setting up Multi-Catalog Environment...\")\n",
        "\n",
        "# Create additional catalogs\n",
        "create_catalogs_sql = \"\"\"\n",
        "-- Create Delta Lake catalog\n",
        "CREATE CATALOG IF NOT EXISTS delta_catalog\n",
        "WITH (\n",
        "    CATALOG_TYPE = 'DELTA',\n",
        "    CATALOG_PROVIDER = 'DATABRICKS',\n",
        "    CATALOG_SOURCE = 'EXTERNAL'\n",
        ");\n",
        "\n",
        "-- Create Hive catalog\n",
        "CREATE CATALOG IF NOT EXISTS hive_catalog\n",
        "WITH (\n",
        "    CATALOG_TYPE = 'HIVE',\n",
        "    CATALOG_PROVIDER = 'HADOOP',\n",
        "    CATALOG_SOURCE = 'EXTERNAL'\n",
        ");\n",
        "\n",
        "-- Create unified catalog\n",
        "CREATE CATALOG IF NOT EXISTS unified_catalog\n",
        "WITH (\n",
        "    CATALOG_TYPE = 'UNIFIED',\n",
        "    CATALOG_PROVIDER = 'SNOWFLAKE',\n",
        "    CATALOG_SOURCE = 'SNOWFLAKE'\n",
        ");\n",
        "\"\"\"\n",
        "execute_sql(conn, create_catalogs_sql, \"Multiple catalogs created\")\n",
        "\n",
        "# 2. Catalog Discovery and Registration\n",
        "print(\"\\nSetting up Catalog Discovery...\")\n",
        "\n",
        "# Register external data sources\n",
        "register_sources_sql = \"\"\"\n",
        "-- Register S3 data source\n",
        "CREATE OR REPLACE EXTERNAL STAGE s3_data_source\n",
        "URL = 's3://my-data-bucket/'\n",
        "CREDENTIALS = (AWS_KEY_ID = 'your_key' AWS_SECRET_KEY = 'your_secret');\n",
        "\n",
        "-- Register Azure data source\n",
        "CREATE OR REPLACE EXTERNAL STAGE azure_data_source\n",
        "URL = 'azure://my-storage-account.blob.core.windows.net/my-container/'\n",
        "CREDENTIALS = (AZURE_SAS_TOKEN = 'your_sas_token');\n",
        "\n",
        "-- Register GCS data source\n",
        "CREATE OR REPLACE EXTERNAL STAGE gcs_data_source\n",
        "URL = 'gcs://my-bucket/'\n",
        "CREDENTIALS = (GCS_SERVICE_ACCOUNT_KEY = 'your_service_account_key');\n",
        "\"\"\"\n",
        "execute_sql(conn, register_sources_sql, \"External data sources registered\")\n",
        "\n",
        "# 3. Catalog Federation\n",
        "print(\"\\nSetting up Catalog Federation...\")\n",
        "\n",
        "# Create federated views across catalogs\n",
        "create_federated_views_sql = \"\"\"\n",
        "-- Create federated view across Iceberg and Delta catalogs\n",
        "CREATE OR REPLACE VIEW federated.sales_unified AS\n",
        "SELECT \n",
        "    'iceberg' as source_catalog,\n",
        "    order_id,\n",
        "    customer_id,\n",
        "    order_date,\n",
        "    total_amount\n",
        "FROM iceberg_catalog.sales_data\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT \n",
        "    'delta' as source_catalog,\n",
        "    order_id,\n",
        "    customer_id,\n",
        "    order_date,\n",
        "    total_amount\n",
        "FROM delta_catalog.sales_data;\n",
        "\n",
        "-- Create cross-catalog analytics view\n",
        "CREATE OR REPLACE VIEW federated.cross_catalog_analytics AS\n",
        "SELECT \n",
        "    source_catalog,\n",
        "    COUNT(*) as total_orders,\n",
        "    SUM(total_amount) as total_revenue,\n",
        "    AVG(total_amount) as avg_order_value\n",
        "FROM federated.sales_unified\n",
        "GROUP BY source_catalog;\n",
        "\"\"\"\n",
        "execute_sql(conn, create_federated_views_sql, \"Federated views created\")\n",
        "\n",
        "# 4. Metadata Synchronization\n",
        "print(\"\\nSetting up Metadata Synchronization...\")\n",
        "\n",
        "# Create metadata sync table\n",
        "create_metadata_sync_sql = \"\"\"\n",
        "CREATE OR REPLACE TABLE catalog_management.metadata_sync (\n",
        "    sync_id STRING PRIMARY KEY,\n",
        "    source_catalog STRING,\n",
        "    target_catalog STRING,\n",
        "    sync_type STRING,\n",
        "    last_sync_time TIMESTAMP,\n",
        "    sync_status STRING,\n",
        "    records_synced INTEGER,\n",
        "    sync_duration_ms INTEGER\n",
        ");\n",
        "\"\"\"\n",
        "execute_sql(conn, create_metadata_sync_sql, \"Metadata sync table created\")\n",
        "\n",
        "# 5. Catalog Monitoring\n",
        "print(\"\\nSetting up Catalog Monitoring...\")\n",
        "\n",
        "# Create catalog monitoring view\n",
        "catalog_monitoring_sql = \"\"\"\n",
        "CREATE OR REPLACE VIEW catalog_management.catalog_monitoring AS\n",
        "SELECT \n",
        "    'Catalog Usage' as metric_name,\n",
        "    catalog_name,\n",
        "    COUNT(*) as total_queries,\n",
        "    SUM(query_duration_ms) as total_duration_ms,\n",
        "    AVG(query_duration_ms) as avg_duration_ms\n",
        "FROM snowflake.account_usage.query_history\n",
        "WHERE start_time >= DATEADD(day, -7, CURRENT_DATE())\n",
        "  AND catalog_name IS NOT NULL\n",
        "GROUP BY catalog_name\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT \n",
        "    'Catalog Objects' as metric_name,\n",
        "    catalog_name,\n",
        "    COUNT(*) as total_objects,\n",
        "    COUNT(DISTINCT schema_name) as unique_schemas,\n",
        "    COUNT(DISTINCT table_name) as unique_tables\n",
        "FROM snowflake.information_schema.tables\n",
        "WHERE catalog_name IS NOT NULL\n",
        "GROUP BY catalog_name;\n",
        "\"\"\"\n",
        "execute_sql(conn, catalog_monitoring_sql, \"Catalog monitoring view created\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Enhanced Conclusion & Summary\n",
        "\n",
        "This comprehensive notebook now covers **ALL** major features of Snowflake's Horizon Catalog, including the advanced data governance and data metric functions capabilities. The notebook provides practical examples and best practices for implementing a complete data platform.\n",
        "\n",
        "## Complete Feature Coverage:\n",
        "\n",
        "### **Core Data Management (Sections 1-3)**\n",
        "- **Iceberg Tables**: ACID transactions, schema evolution, time travel\n",
        "- **Traditional Tables & Views**: All table types, materialized views, dynamic tables\n",
        "- **AI & ML Objects**: Model registry, ML functions, vector embeddings\n",
        "\n",
        "### **Operations & Monitoring (Sections 4-6)**\n",
        "- **Costs & Usage Monitoring**: Comprehensive cost tracking and optimization\n",
        "- **Data Warehouses**: Multi-cluster auto-scaling, resource monitors\n",
        "- **Data Shares**: Secure real-time data sharing with governance\n",
        "\n",
        "### **Business Intelligence (Sections 7-9)**\n",
        "- **Semantic Views**: Business-friendly data abstractions\n",
        "- **Databases & Schemas**: Logical organization and data classification\n",
        "- **Business Glossary**: Centralized business vocabulary and lineage\n",
        "\n",
        "### **Automation & Security (Sections 10-12)**\n",
        "- **Workflows**: Automated data pipelines with task orchestration\n",
        "- **RBAC**: Fine-grained security with dynamic masking and row-level security\n",
        "- **Audit Trail & Logging**: Comprehensive activity logging and compliance\n",
        "\n",
        "### **Data Discovery & Quality (Sections 13-15)**\n",
        "- **Metadata Retrieval**: Data discovery, relationship mapping, usage analytics\n",
        "- **Data Quality**: Validation rules, automated monitoring, quality metrics\n",
        "- **Practical Examples**: Real-world implementations and optimization strategies\n",
        "\n",
        "### **Advanced Features (Sections 16-18)**\n",
        "- **Data Metric Functions (DMFs)**: Advanced data quality monitoring with system and custom DMFs\n",
        "- **Advanced Data Governance**: Sensitive data classification, lineage tracking, compliance\n",
        "- **Catalog Management**: Multi-catalog support, federation, metadata synchronization\n",
        "\n",
        "## Key Benefits of Horizon Catalog:\n",
        "\n",
        "### **Enterprise-Grade Data Quality**\n",
        "- **Automated Monitoring**: DMFs provide continuous data quality assessment\n",
        "- **Custom Metrics**: Define business-specific quality measurements\n",
        "- **Expectations**: Set pass/fail criteria for automated quality checks\n",
        "- **Scheduling**: Regular quality checks without manual intervention\n",
        "\n",
        "### **Comprehensive Data Governance**\n",
        "- **Sensitive Data Classification**: Automatic detection and tagging of PII\n",
        "- **Data Lineage**: Complete visibility into data flow and transformations\n",
        "- **Access Policies**: Advanced security controls including aggregation policies\n",
        "- **Compliance Reporting**: Automated compliance and audit reporting\n",
        "\n",
        "### **Multi-Catalog Management**\n",
        "- **Federation**: Query across multiple data sources seamlessly\n",
        "- **Discovery**: Automatic discovery and registration of data sources\n",
        "- **Synchronization**: Keep metadata in sync across different catalogs\n",
        "- **Monitoring**: Track usage and performance across all catalogs\n",
        "\n",
        "## Implementation Roadmap:\n",
        "\n",
        "### **Phase 1: Foundation (Weeks 1-4)**\n",
        "1. Set up basic Iceberg and traditional tables\n",
        "2. Implement basic RBAC and security policies\n",
        "3. Create semantic views for business users\n",
        "4. Set up basic cost monitoring\n",
        "\n",
        "### **Phase 2: Quality & Governance (Weeks 5-8)**\n",
        "1. Implement DMFs for data quality monitoring\n",
        "2. Set up sensitive data classification\n",
        "3. Create data lineage documentation\n",
        "4. Implement advanced access policies\n",
        "\n",
        "### **Phase 3: Advanced Features (Weeks 9-12)**\n",
        "1. Set up multi-catalog federation\n",
        "2. Implement automated workflows\n",
        "3. Create comprehensive compliance reporting\n",
        "4. Set up advanced monitoring and alerting\n",
        "\n",
        "## Resources and Documentation:\n",
        "\n",
        "- **Snowflake Data Quality Documentation**: Comprehensive guide to DMFs\n",
        "- **Snowflake Documentation**: Complete Snowflake platform documentation\n",
        "- **Horizon Catalog Guide**: Official Horizon Catalog documentation\n",
        "- **Best Practices Guide**: Implementation best practices\n",
        "- **Security Guide**: Security implementation guide\n",
        "\n",
        "## Next Steps:\n",
        "\n",
        "1. **Start with Core Features**: Begin with Iceberg tables and basic governance\n",
        "2. **Implement DMFs**: Set up data quality monitoring early in your implementation\n",
        "3. **Establish Governance**: Create data classification and lineage tracking\n",
        "4. **Scale Gradually**: Add advanced features as your needs grow\n",
        "5. **Monitor and Optimize**: Continuously monitor performance and costs\n",
        "\n",
        "---\n",
        "\n",
        "**Note**: This notebook provides a complete implementation guide for Snowflake Horizon Catalog. Remember to update the connection configuration and adapt the examples to your specific environment and requirements.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 19. Business Vocabulary & Data Dictionary\n",
        "\n",
        "A comprehensive business vocabulary provides standardized definitions for business terms, metrics, and concepts, enabling better communication between business and technical teams while ensuring data consistency across the organization.\n",
        "\n",
        "## Business Vocabulary Features:\n",
        "- **Term Definitions**: Centralized business terminology\n",
        "- **Metric Definitions**: Standardized calculation methods\n",
        "- **Data Mappings**: Link business terms to data objects\n",
        "- **Business Rules**: Document business logic and constraints\n",
        "- **Term Relationships**: Define relationships between concepts\n",
        "- **Version Control**: Track changes to business definitions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Business Vocabulary & Data Dictionary Examples\n",
        "\n",
        "# 1. Create Business Vocabulary Schema\n",
        "print(\"Creating Business Vocabulary Framework...\")\n",
        "\n",
        "# Create comprehensive business vocabulary schema\n",
        "create_vocabulary_schema_sql = \"\"\"\n",
        "CREATE OR REPLACE SCHEMA business_vocabulary;\n",
        "\n",
        "-- Business Terms Dictionary\n",
        "CREATE OR REPLACE TABLE business_vocabulary.business_terms (\n",
        "    term_id STRING PRIMARY KEY,\n",
        "    term_name STRING NOT NULL,\n",
        "    definition STRING NOT NULL,\n",
        "    category STRING,\n",
        "    subcategory STRING,\n",
        "    business_domain STRING,\n",
        "    data_type STRING,\n",
        "    calculation_method STRING,\n",
        "    business_owner STRING,\n",
        "    data_steward STRING,\n",
        "    created_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),\n",
        "    updated_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),\n",
        "    version STRING DEFAULT '1.0',\n",
        "    status STRING DEFAULT 'ACTIVE',\n",
        "    approved_by STRING,\n",
        "    approval_date TIMESTAMP\n",
        ");\n",
        "\n",
        "-- Business Metrics Dictionary\n",
        "CREATE OR REPLACE TABLE business_vocabulary.business_metrics (\n",
        "    metric_id STRING PRIMARY KEY,\n",
        "    metric_name STRING NOT NULL,\n",
        "    metric_definition STRING NOT NULL,\n",
        "    calculation_formula STRING,\n",
        "    data_source STRING,\n",
        "    frequency STRING,\n",
        "    unit_of_measure STRING,\n",
        "    business_owner STRING,\n",
        "    technical_owner STRING,\n",
        "    created_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),\n",
        "    updated_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),\n",
        "    version STRING DEFAULT '1.0',\n",
        "    status STRING DEFAULT 'ACTIVE'\n",
        ");\n",
        "\n",
        "-- Data Object Mappings\n",
        "CREATE OR REPLACE TABLE business_vocabulary.data_mappings (\n",
        "    mapping_id STRING PRIMARY KEY,\n",
        "    business_term_id STRING,\n",
        "    data_object_name STRING,\n",
        "    data_object_type STRING,\n",
        "    column_name STRING,\n",
        "    mapping_type STRING,\n",
        "    transformation_rule STRING,\n",
        "    created_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),\n",
        "    FOREIGN KEY (business_term_id) REFERENCES business_vocabulary.business_terms(term_id)\n",
        ");\n",
        "\n",
        "-- Term Relationships\n",
        "CREATE OR REPLACE TABLE business_vocabulary.term_relationships (\n",
        "    relationship_id STRING PRIMARY KEY,\n",
        "    source_term_id STRING,\n",
        "    target_term_id STRING,\n",
        "    relationship_type STRING,\n",
        "    relationship_description STRING,\n",
        "    created_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),\n",
        "    FOREIGN KEY (source_term_id) REFERENCES business_vocabulary.business_terms(term_id),\n",
        "    FOREIGN KEY (target_term_id) REFERENCES business_vocabulary.business_terms(term_id)\n",
        ");\n",
        "\"\"\"\n",
        "execute_sql(conn, create_vocabulary_schema_sql, \"Business vocabulary framework created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Business Terms and Definitions\n",
        "\n",
        "# 2. Insert Comprehensive Business Terms\n",
        "print(\"\\nAdding Business Terms...\")\n",
        "\n",
        "# Insert core business terms\n",
        "insert_business_terms_sql = \"\"\"\n",
        "INSERT INTO business_vocabulary.business_terms VALUES\n",
        "-- Customer Analytics Terms\n",
        "('TERM001', 'Customer', 'An individual or organization that purchases products or services from the company', 'Customer Analytics', 'Customer Management', 'Sales', 'Entity', 'Business Definition', 'sales_manager', 'data_analyst', CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP(), '1.0', 'ACTIVE', 'cfo', CURRENT_TIMESTAMP()),\n",
        "\n",
        "('TERM002', 'Customer Lifetime Value (CLV)', 'Total revenue generated by a customer over their entire relationship with the company', 'Customer Analytics', 'Customer Value', 'Sales', 'Metric', 'SUM(total_amount) WHERE customer_id = X', 'marketing_manager', 'data_scientist', CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP(), '1.0', 'ACTIVE', 'cfo', CURRENT_TIMESTAMP()),\n",
        "\n",
        "('TERM003', 'Customer Acquisition Cost (CAC)', 'Total cost of acquiring a new customer, including marketing and sales expenses', 'Customer Analytics', 'Customer Acquisition', 'Marketing', 'Metric', 'Total Marketing Spend / Number of New Customers', 'marketing_manager', 'data_analyst', CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP(), '1.0', 'ACTIVE', 'cfo', CURRENT_TIMESTAMP()),\n",
        "\n",
        "('TERM004', 'Churn Rate', 'Percentage of customers who stop using the service within a given time period', 'Customer Analytics', 'Customer Retention', 'Customer Success', 'Metric', '(Customers Lost / Total Customers) * 100', 'customer_success_manager', 'data_scientist', CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP(), '1.0', 'ACTIVE', 'cfo', CURRENT_TIMESTAMP()),\n",
        "\n",
        "('TERM005', 'Customer Segment', 'A group of customers with similar characteristics, behaviors, or needs', 'Customer Analytics', 'Customer Segmentation', 'Marketing', 'Entity', 'Business Definition', 'marketing_manager', 'data_analyst', CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP(), '1.0', 'ACTIVE', 'cfo', CURRENT_TIMESTAMP()),\n",
        "\n",
        "-- Sales Terms\n",
        "('TERM006', 'Order', 'A request from a customer to purchase products or services', 'Sales', 'Order Management', 'Sales', 'Entity', 'Business Definition', 'sales_manager', 'data_analyst', CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP(), '1.0', 'ACTIVE', 'cfo', CURRENT_TIMESTAMP()),\n",
        "\n",
        "('TERM007', 'Revenue', 'Total income generated from sales of products or services', 'Sales', 'Financial Metrics', 'Finance', 'Metric', 'SUM(order_amount)', 'finance_manager', 'data_analyst', CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP(), '1.0', 'ACTIVE', 'cfo', CURRENT_TIMESTAMP()),\n",
        "\n",
        "('TERM008', 'Average Order Value (AOV)', 'Average amount spent per order', 'Sales', 'Order Metrics', 'Sales', 'Metric', 'SUM(total_amount) / COUNT(orders)', 'sales_manager', 'data_analyst', CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP(), '1.0', 'ACTIVE', 'cfo', CURRENT_TIMESTAMP()),\n",
        "\n",
        "('TERM009', 'Sales Conversion Rate', 'Percentage of leads or prospects that convert to paying customers', 'Sales', 'Conversion Metrics', 'Sales', 'Metric', '(Converted Leads / Total Leads) * 100', 'sales_manager', 'data_analyst', CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP(), '1.0', 'ACTIVE', 'cfo', CURRENT_TIMESTAMP()),\n",
        "\n",
        "-- Product Terms\n",
        "('TERM010', 'Product', 'A tangible or intangible item that is offered for sale', 'Product Management', 'Product Catalog', 'Product', 'Entity', 'Business Definition', 'product_manager', 'data_analyst', CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP(), '1.0', 'ACTIVE', 'cfo', CURRENT_TIMESTAMP()),\n",
        "\n",
        "('TERM011', 'Product Category', 'A classification of products based on their characteristics or use', 'Product Management', 'Product Classification', 'Product', 'Entity', 'Business Definition', 'product_manager', 'data_analyst', CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP(), '1.0', 'ACTIVE', 'cfo', CURRENT_TIMESTAMP()),\n",
        "\n",
        "('TERM012', 'Inventory Turnover', 'Number of times inventory is sold and replaced in a given period', 'Product Management', 'Inventory Metrics', 'Operations', 'Metric', 'Cost of Goods Sold / Average Inventory', 'operations_manager', 'data_analyst', CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP(), '1.0', 'ACTIVE', 'cfo', CURRENT_TIMESTAMP()),\n",
        "\n",
        "-- Financial Terms\n",
        "('TERM013', 'Gross Profit', 'Revenue minus cost of goods sold', 'Finance', 'Profitability', 'Finance', 'Metric', 'Revenue - Cost of Goods Sold', 'finance_manager', 'data_analyst', CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP(), '1.0', 'ACTIVE', 'cfo', CURRENT_TIMESTAMP()),\n",
        "\n",
        "('TERM014', 'Net Profit Margin', 'Percentage of revenue that remains as profit after all expenses', 'Finance', 'Profitability', 'Finance', 'Metric', '(Net Profit / Revenue) * 100', 'finance_manager', 'data_analyst', CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP(), '1.0', 'ACTIVE', 'cfo', CURRENT_TIMESTAMP()),\n",
        "\n",
        "('TERM015', 'Monthly Recurring Revenue (MRR)', 'Total predictable revenue generated from subscriptions each month', 'Finance', 'Recurring Revenue', 'Finance', 'Metric', 'SUM(monthly_subscription_amount)', 'finance_manager', 'data_analyst', CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP(), '1.0', 'ACTIVE', 'cfo', CURRENT_TIMESTAMP());\n",
        "\"\"\"\n",
        "execute_sql(conn, insert_business_terms_sql, \"Business terms added\")\n",
        "\n",
        "# 3. Insert Business Metrics\n",
        "print(\"\\nAdding Business Metrics...\")\n",
        "\n",
        "insert_business_metrics_sql = \"\"\"\n",
        "INSERT INTO business_vocabulary.business_metrics VALUES\n",
        "('METRIC001', 'Customer Lifetime Value', 'Total revenue generated by a customer over their entire relationship', 'SUM(total_amount) WHERE customer_id = X', 'customers, sales_data', 'Monthly', 'USD', 'marketing_manager', 'data_scientist', CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP(), '1.0', 'ACTIVE'),\n",
        "\n",
        "('METRIC002', 'Customer Acquisition Cost', 'Total cost of acquiring a new customer', 'Total Marketing Spend / Number of New Customers', 'marketing_data, customer_data', 'Monthly', 'USD', 'marketing_manager', 'data_analyst', CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP(), '1.0', 'ACTIVE'),\n",
        "\n",
        "('METRIC003', 'Churn Rate', 'Percentage of customers who stop using the service', '(Customers Lost / Total Customers) * 100', 'customers', 'Monthly', 'Percentage', 'customer_success_manager', 'data_scientist', CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP(), '1.0', 'ACTIVE'),\n",
        "\n",
        "('METRIC004', 'Average Order Value', 'Average amount spent per order', 'SUM(total_amount) / COUNT(orders)', 'sales_data', 'Daily', 'USD', 'sales_manager', 'data_analyst', CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP(), '1.0', 'ACTIVE'),\n",
        "\n",
        "('METRIC005', 'Sales Conversion Rate', 'Percentage of leads that convert to customers', '(Converted Leads / Total Leads) * 100', 'leads_data, customers', 'Weekly', 'Percentage', 'sales_manager', 'data_analyst', CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP(), '1.0', 'ACTIVE'),\n",
        "\n",
        "('METRIC006', 'Monthly Recurring Revenue', 'Total predictable revenue from subscriptions', 'SUM(monthly_subscription_amount)', 'subscription_data', 'Monthly', 'USD', 'finance_manager', 'data_analyst', CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP(), '1.0', 'ACTIVE'),\n",
        "\n",
        "('METRIC007', 'Gross Profit Margin', 'Percentage of revenue remaining after COGS', '(Gross Profit / Revenue) * 100', 'sales_data, cost_data', 'Monthly', 'Percentage', 'finance_manager', 'data_analyst', CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP(), '1.0', 'ACTIVE'),\n",
        "\n",
        "('METRIC008', 'Inventory Turnover', 'Number of times inventory is sold and replaced', 'Cost of Goods Sold / Average Inventory', 'inventory_data, sales_data', 'Monthly', 'Ratio', 'operations_manager', 'data_analyst', CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP(), '1.0', 'ACTIVE');\n",
        "\"\"\"\n",
        "execute_sql(conn, insert_business_metrics_sql, \"Business metrics added\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Mappings and Term Relationships\n",
        "\n",
        "# 4. Create Data Object Mappings\n",
        "print(\"\\nCreating Data Object Mappings...\")\n",
        "\n",
        "insert_data_mappings_sql = \"\"\"\n",
        "INSERT INTO business_vocabulary.data_mappings VALUES\n",
        "-- Customer mappings\n",
        "('MAP001', 'TERM001', 'customers', 'TABLE', 'customer_id', 'Direct', 'Primary key for customer identification', CURRENT_TIMESTAMP()),\n",
        "('MAP002', 'TERM001', 'customers', 'TABLE', 'first_name', 'Direct', 'Customer first name', CURRENT_TIMESTAMP()),\n",
        "('MAP003', 'TERM001', 'customers', 'TABLE', 'last_name', 'Direct', 'Customer last name', CURRENT_TIMESTAMP()),\n",
        "('MAP004', 'TERM001', 'customers', 'TABLE', 'email', 'Direct', 'Customer email address', CURRENT_TIMESTAMP()),\n",
        "\n",
        "-- Order mappings\n",
        "('MAP005', 'TERM006', 'iceberg_catalog.sales_data', 'TABLE', 'order_id', 'Direct', 'Unique identifier for each order', CURRENT_TIMESTAMP()),\n",
        "('MAP006', 'TERM006', 'iceberg_catalog.sales_data', 'TABLE', 'customer_id', 'Foreign Key', 'Reference to customer who placed the order', CURRENT_TIMESTAMP()),\n",
        "('MAP007', 'TERM006', 'iceberg_catalog.sales_data', 'TABLE', 'order_date', 'Direct', 'Date when the order was placed', CURRENT_TIMESTAMP()),\n",
        "('MAP008', 'TERM006', 'iceberg_catalog.sales_data', 'TABLE', 'total_amount', 'Direct', 'Total amount of the order', CURRENT_TIMESTAMP()),\n",
        "\n",
        "-- Product mappings\n",
        "('MAP009', 'TERM010', 'staging_products', 'TABLE', 'product_id', 'Direct', 'Unique identifier for each product', CURRENT_TIMESTAMP()),\n",
        "('MAP010', 'TERM010', 'staging_products', 'TABLE', 'product_name', 'Direct', 'Name of the product', CURRENT_TIMESTAMP()),\n",
        "('MAP011', 'TERM011', 'staging_products', 'TABLE', 'category', 'Direct', 'Product category classification', CURRENT_TIMESTAMP()),\n",
        "('MAP012', 'TERM010', 'staging_products', 'TABLE', 'price', 'Direct', 'Price of the product', CURRENT_TIMESTAMP()),\n",
        "\n",
        "-- Calculated metrics mappings\n",
        "('MAP013', 'TERM002', 'customer_kpi', 'VIEW', 'lifetime_value', 'Calculated', 'SUM(total_amount) for each customer', CURRENT_TIMESTAMP()),\n",
        "('MAP014', 'TERM008', 'customer_kpi', 'VIEW', 'avg_order_value', 'Calculated', 'AVG(total_amount) for each customer', CURRENT_TIMESTAMP()),\n",
        "('MAP015', 'TERM007', 'revenue_analytics', 'VIEW', 'total_revenue', 'Calculated', 'SUM(total_amount) by month', CURRENT_TIMESTAMP());\n",
        "\"\"\"\n",
        "execute_sql(conn, insert_data_mappings_sql, \"Data mappings created\")\n",
        "\n",
        "# 5. Create Term Relationships\n",
        "print(\"\\nCreating Term Relationships...\")\n",
        "\n",
        "insert_term_relationships_sql = \"\"\"\n",
        "INSERT INTO business_vocabulary.term_relationships VALUES\n",
        "-- Customer-related relationships\n",
        "('REL001', 'TERM001', 'TERM002', 'CALCULATES', 'Customer entity is used to calculate Customer Lifetime Value', CURRENT_TIMESTAMP()),\n",
        "('REL001', 'TERM001', 'TERM003', 'CALCULATES', 'Customer entity is used to calculate Customer Acquisition Cost', CURRENT_TIMESTAMP()),\n",
        "('REL001', 'TERM001', 'TERM004', 'CALCULATES', 'Customer entity is used to calculate Churn Rate', CURRENT_TIMESTAMP()),\n",
        "('REL001', 'TERM001', 'TERM005', 'CLASSIFIES', 'Customer entity is classified into Customer Segments', CURRENT_TIMESTAMP()),\n",
        "\n",
        "-- Order-related relationships\n",
        "('REL002', 'TERM006', 'TERM007', 'CALCULATES', 'Order entity is used to calculate Revenue', CURRENT_TIMESTAMP()),\n",
        "('REL002', 'TERM006', 'TERM008', 'CALCULATES', 'Order entity is used to calculate Average Order Value', CURRENT_TIMESTAMP()),\n",
        "('REL002', 'TERM001', 'TERM006', 'PLACES', 'Customer places Orders', CURRENT_TIMESTAMP()),\n",
        "('REL002', 'TERM010', 'TERM006', 'CONTAINS', 'Order contains Products', CURRENT_TIMESTAMP()),\n",
        "\n",
        "-- Product-related relationships\n",
        "('REL003', 'TERM010', 'TERM011', 'CLASSIFIED_BY', 'Product is classified by Product Category', CURRENT_TIMESTAMP()),\n",
        "('REL003', 'TERM010', 'TERM012', 'CALCULATES', 'Product entity is used to calculate Inventory Turnover', CURRENT_TIMESTAMP()),\n",
        "\n",
        "-- Financial relationships\n",
        "('REL004', 'TERM007', 'TERM013', 'CALCULATES', 'Revenue is used to calculate Gross Profit', CURRENT_TIMESTAMP()),\n",
        "('REL004', 'TERM013', 'TERM014', 'CALCULATES', 'Gross Profit is used to calculate Net Profit Margin', CURRENT_TIMESTAMP()),\n",
        "('REL004', 'TERM015', 'TERM007', 'SUBSET_OF', 'Monthly Recurring Revenue is a subset of total Revenue', CURRENT_TIMESTAMP());\n",
        "\"\"\"\n",
        "execute_sql(conn, insert_term_relationships_sql, \"Term relationships created\")\n",
        "\n",
        "# 6. Create Business Vocabulary Views\n",
        "print(\"\\nCreating Business Vocabulary Views...\")\n",
        "\n",
        "create_vocabulary_views_sql = \"\"\"\n",
        "-- Business Terms Summary View\n",
        "CREATE OR REPLACE VIEW business_vocabulary.terms_summary AS\n",
        "SELECT \n",
        "    bt.term_name,\n",
        "    bt.definition,\n",
        "    bt.category,\n",
        "    bt.subcategory,\n",
        "    bt.business_domain,\n",
        "    bt.business_owner,\n",
        "    bt.data_steward,\n",
        "    bt.status,\n",
        "    bt.version,\n",
        "    COUNT(dm.mapping_id) as data_mappings_count\n",
        "FROM business_vocabulary.business_terms bt\n",
        "LEFT JOIN business_vocabulary.data_mappings dm ON bt.term_id = dm.business_term_id\n",
        "GROUP BY bt.term_id, bt.term_name, bt.definition, bt.category, bt.subcategory, \n",
        "         bt.business_domain, bt.business_owner, bt.data_steward, bt.status, bt.version;\n",
        "\n",
        "-- Metrics Summary View\n",
        "CREATE OR REPLACE VIEW business_vocabulary.metrics_summary AS\n",
        "SELECT \n",
        "    bm.metric_name,\n",
        "    bm.metric_definition,\n",
        "    bm.calculation_formula,\n",
        "    bm.data_source,\n",
        "    bm.frequency,\n",
        "    bm.unit_of_measure,\n",
        "    bm.business_owner,\n",
        "    bm.technical_owner,\n",
        "    bm.status,\n",
        "    bm.version\n",
        "FROM business_vocabulary.business_metrics bm;\n",
        "\n",
        "-- Data Lineage View\n",
        "CREATE OR REPLACE VIEW business_vocabulary.data_lineage_view AS\n",
        "SELECT \n",
        "    bt.term_name,\n",
        "    bt.definition,\n",
        "    dm.data_object_name,\n",
        "    dm.data_object_type,\n",
        "    dm.column_name,\n",
        "    dm.mapping_type,\n",
        "    dm.transformation_rule\n",
        "FROM business_vocabulary.business_terms bt\n",
        "JOIN business_vocabulary.data_mappings dm ON bt.term_id = dm.business_term_id\n",
        "ORDER BY bt.term_name, dm.data_object_name;\n",
        "\"\"\"\n",
        "execute_sql(conn, create_vocabulary_views_sql, \"Business vocabulary views created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Business Vocabulary Analytics and Reporting\n",
        "\n",
        "# 7. Create Business Vocabulary Analytics\n",
        "print(\"\\nCreating Business Vocabulary Analytics...\")\n",
        "\n",
        "# Query business terms by category\n",
        "business_terms_analytics_sql = \"\"\"\n",
        "SELECT \n",
        "    category,\n",
        "    subcategory,\n",
        "    COUNT(*) as term_count,\n",
        "    COUNT(CASE WHEN status = 'ACTIVE' THEN 1 END) as active_terms,\n",
        "    COUNT(DISTINCT business_owner) as unique_owners,\n",
        "    COUNT(DISTINCT data_steward) as unique_stewards\n",
        "FROM business_vocabulary.business_terms\n",
        "GROUP BY category, subcategory\n",
        "ORDER BY category, subcategory;\n",
        "\"\"\"\n",
        "df_business_terms_analytics = execute_sql(conn, business_terms_analytics_sql, \"Business terms analytics completed\")\n",
        "\n",
        "# Query metrics by frequency\n",
        "metrics_analytics_sql = \"\"\"\n",
        "SELECT \n",
        "    frequency,\n",
        "    unit_of_measure,\n",
        "    COUNT(*) as metric_count,\n",
        "    COUNT(DISTINCT business_owner) as unique_owners,\n",
        "    COUNT(DISTINCT technical_owner) as unique_technical_owners\n",
        "FROM business_vocabulary.business_metrics\n",
        "GROUP BY frequency, unit_of_measure\n",
        "ORDER BY frequency, unit_of_measure;\n",
        "\"\"\"\n",
        "df_metrics_analytics = execute_sql(conn, metrics_analytics_sql, \"Metrics analytics completed\")\n",
        "\n",
        "# Query data mappings by object type\n",
        "mappings_analytics_sql = \"\"\"\n",
        "SELECT \n",
        "    dm.data_object_type,\n",
        "    dm.mapping_type,\n",
        "    COUNT(*) as mapping_count,\n",
        "    COUNT(DISTINCT dm.data_object_name) as unique_objects,\n",
        "    COUNT(DISTINCT dm.business_term_id) as unique_terms\n",
        "FROM business_vocabulary.data_mappings dm\n",
        "GROUP BY dm.data_object_type, dm.mapping_type\n",
        "ORDER BY dm.data_object_type, dm.mapping_type;\n",
        "\"\"\"\n",
        "df_mappings_analytics = execute_sql(conn, mappings_analytics_sql, \"Mappings analytics completed\")\n",
        "\n",
        "# 8. Create Business Vocabulary Dashboard\n",
        "print(\"\\nCreating Business Vocabulary Dashboard...\")\n",
        "\n",
        "create_dashboard_sql = \"\"\"\n",
        "CREATE OR REPLACE VIEW business_vocabulary.vocabulary_dashboard AS\n",
        "SELECT \n",
        "    'Business Terms' as vocabulary_type,\n",
        "    COUNT(*) as total_items,\n",
        "    COUNT(CASE WHEN status = 'ACTIVE' THEN 1 END) as active_items,\n",
        "    COUNT(DISTINCT category) as unique_categories,\n",
        "    COUNT(DISTINCT business_owner) as unique_owners\n",
        "FROM business_vocabulary.business_terms\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT \n",
        "    'Business Metrics' as vocabulary_type,\n",
        "    COUNT(*) as total_items,\n",
        "    COUNT(CASE WHEN status = 'ACTIVE' THEN 1 END) as active_items,\n",
        "    COUNT(DISTINCT frequency) as unique_categories,\n",
        "    COUNT(DISTINCT business_owner) as unique_owners\n",
        "FROM business_vocabulary.business_metrics\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT \n",
        "    'Data Mappings' as vocabulary_type,\n",
        "    COUNT(*) as total_items,\n",
        "    COUNT(*) as active_items,\n",
        "    COUNT(DISTINCT data_object_type) as unique_categories,\n",
        "    COUNT(DISTINCT business_term_id) as unique_owners\n",
        "FROM business_vocabulary.data_mappings;\n",
        "\"\"\"\n",
        "execute_sql(conn, create_dashboard_sql, \"Business vocabulary dashboard created\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final Enhanced Conclusion & Summary\n",
        "\n",
        "This comprehensive notebook now covers **ALL** major features of Snowflake's Horizon Catalog, including advanced data governance, data metric functions, and comprehensive business vocabulary capabilities. The notebook provides a complete enterprise-ready implementation guide for modern data platforms.\n",
        "\n",
        "## Complete Feature Coverage (19 Sections):\n",
        "\n",
        "### **Core Data Management (Sections 1-3)**\n",
        "- **Iceberg Tables**: ACID transactions, schema evolution, time travel\n",
        "- **Traditional Tables & Views**: All table types, materialized views, dynamic tables\n",
        "- **AI & ML Objects**: Model registry, ML functions, vector embeddings\n",
        "\n",
        "### **Operations & Monitoring (Sections 4-6)**\n",
        "- **Costs & Usage Monitoring**: Comprehensive cost tracking and optimization\n",
        "- **Data Warehouses**: Multi-cluster auto-scaling, resource monitors\n",
        "- **Data Shares**: Secure real-time data sharing with governance\n",
        "\n",
        "### **Business Intelligence (Sections 7-9)**\n",
        "- **Semantic Views**: Business-friendly data abstractions\n",
        "- **Databases & Schemas**: Logical organization and data classification\n",
        "- **Business Glossary**: Centralized business vocabulary and lineage\n",
        "\n",
        "### **Automation & Security (Sections 10-12)**\n",
        "- **Workflows**: Automated data pipelines with task orchestration\n",
        "- **RBAC**: Fine-grained security with dynamic masking and row-level security\n",
        "- **Audit Trail & Logging**: Comprehensive activity logging and compliance\n",
        "\n",
        "### **Data Discovery & Quality (Sections 13-15)**\n",
        "- **Metadata Retrieval**: Data discovery, relationship mapping, usage analytics\n",
        "- **Data Quality**: Validation rules, automated monitoring, quality metrics\n",
        "- **Practical Examples**: Real-world implementations and optimization strategies\n",
        "\n",
        "### **Advanced Features (Sections 16-18)**\n",
        "- **Data Metric Functions (DMFs)**: Advanced data quality monitoring with system and custom DMFs\n",
        "- **Advanced Data Governance**: Sensitive data classification, lineage tracking, compliance\n",
        "- **Catalog Management**: Multi-catalog support, federation, metadata synchronization\n",
        "\n",
        "### **Business Vocabulary (Section 19)**\n",
        "- **Business Terms Dictionary**: Centralized business terminology with definitions\n",
        "- **Business Metrics**: Standardized calculation methods and formulas\n",
        "- **Data Mappings**: Link business terms to data objects and columns\n",
        "- **Term Relationships**: Define relationships between business concepts\n",
        "- **Vocabulary Analytics**: Search, reporting, and analytics for business vocabulary\n",
        "\n",
        "## Key Benefits of Complete Horizon Catalog Implementation:\n",
        "\n",
        "### **Enterprise-Grade Data Quality**\n",
        "- **Automated Monitoring**: DMFs provide continuous data quality assessment\n",
        "- **Custom Metrics**: Define business-specific quality measurements\n",
        "- **Expectations**: Set pass/fail criteria for automated quality checks\n",
        "- **Scheduling**: Regular quality checks without manual intervention\n",
        "\n",
        "### **Comprehensive Data Governance**\n",
        "- **Sensitive Data Classification**: Automatic detection and tagging of PII\n",
        "- **Data Lineage**: Complete visibility into data flow and transformations\n",
        "- **Access Policies**: Advanced security controls including aggregation policies\n",
        "- **Compliance Reporting**: Automated compliance and audit reporting\n",
        "\n",
        "### **Business Vocabulary Management**\n",
        "- **Standardized Terminology**: Consistent business language across the organization\n",
        "- **Data-Business Alignment**: Clear mapping between business terms and data objects\n",
        "- **Searchable Dictionary**: Easy discovery of business terms and definitions\n",
        "- **Version Control**: Track changes to business definitions over time\n",
        "\n",
        "### **Multi-Catalog Management**\n",
        "- **Federation**: Query across multiple data sources seamlessly\n",
        "- **Discovery**: Automatic discovery and registration of data sources\n",
        "- **Synchronization**: Keep metadata in sync across different catalogs\n",
        "- **Monitoring**: Track usage and performance across all catalogs\n",
        "\n",
        "## Implementation Roadmap:\n",
        "\n",
        "### **Phase 1: Foundation (Weeks 1-4)**\n",
        "1. Set up basic Iceberg and traditional tables\n",
        "2. Implement basic RBAC and security policies\n",
        "3. Create semantic views for business users\n",
        "4. Set up basic cost monitoring\n",
        "\n",
        "### **Phase 2: Quality & Governance (Weeks 5-8)**\n",
        "1. Implement DMFs for data quality monitoring\n",
        "2. Set up sensitive data classification\n",
        "3. Create data lineage documentation\n",
        "4. Implement advanced access policies\n",
        "\n",
        "### **Phase 3: Business Vocabulary (Weeks 9-10)**\n",
        "1. Create comprehensive business terms dictionary\n",
        "2. Map business terms to data objects\n",
        "3. Define business metrics and calculations\n",
        "4. Set up vocabulary search and analytics\n",
        "\n",
        "### **Phase 4: Advanced Features (Weeks 11-12)**\n",
        "1. Set up multi-catalog federation\n",
        "2. Implement automated workflows\n",
        "3. Create comprehensive compliance reporting\n",
        "4. Set up advanced monitoring and alerting\n",
        "\n",
        "## Resources and Documentation:\n",
        "\n",
        "- **Snowflake Data Quality Documentation**: Comprehensive guide to DMFs\n",
        "- **Snowflake Documentation**: Complete Snowflake platform documentation\n",
        "- **Horizon Catalog Guide**: Official Horizon Catalog documentation\n",
        "- **Best Practices Guide**: Implementation best practices\n",
        "- **Security Guide**: Security implementation guide\n",
        "- **Business Vocabulary Guide**: Data governance and business terminology\n",
        "\n",
        "## Next Steps:\n",
        "\n",
        "1. **Start with Core Features**: Begin with Iceberg tables and basic governance\n",
        "2. **Implement DMFs**: Set up data quality monitoring early in your implementation\n",
        "3. **Establish Business Vocabulary**: Create standardized business terminology\n",
        "4. **Scale Gradually**: Add advanced features as your needs grow\n",
        "5. **Monitor and Optimize**: Continuously monitor performance and costs\n",
        "\n",
        "---\n",
        "\n",
        "**Note**: This notebook provides a complete implementation guide for Snowflake Horizon Catalog with comprehensive business vocabulary management. Remember to update the connection configuration and adapt the examples to your specific environment and requirements.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Additional Resources & Learning Path\n",
        "\n",
        "## Recommended Learning Path\n",
        "\n",
        "### **Beginner Level (Weeks 1-2)**\n",
        "1. **Snowflake Fundamentals**: Learn basic SQL and Snowflake concepts\n",
        "2. **Data Types**: Understand Snowflake data types and best practices\n",
        "3. **Basic Queries**: Practice with SELECT, JOIN, and aggregation functions\n",
        "4. **Table Creation**: Learn to create and manage basic tables\n",
        "\n",
        "### **Intermediate Level (Weeks 3-6)**\n",
        "1. **Advanced SQL**: Window functions, CTEs, and complex queries\n",
        "2. **Data Loading**: Bulk loading, streaming, and data transformation\n",
        "3. **Views and Materialized Views**: Create and optimize views\n",
        "4. **Security Basics**: Users, roles, and basic permissions\n",
        "\n",
        "### **Advanced Level (Weeks 7-12)**\n",
        "1. **Horizon Catalog Features**: Implement all features from this notebook\n",
        "2. **Data Governance**: Advanced security, masking, and compliance\n",
        "3. **Performance Optimization**: Query optimization and warehouse sizing\n",
        "4. **Automation**: Tasks, streams, and dynamic tables\n",
        "\n",
        "## Community Resources\n",
        "\n",
        "- **Snowflake Community**: [community.snowflake.com](https://community.snowflake.com)\n",
        "- **Snowflake University**: Free online courses and certifications\n",
        "- **Snowflake Blog**: Latest updates and best practices\n",
        "- **GitHub Examples**: Open-source Snowflake projects and scripts\n",
        "- **Stack Overflow**: Community Q&A for technical questions\n",
        "\n",
        "## Certification Path\n",
        "\n",
        "1. **SnowPro Core Certification**: Fundamental Snowflake knowledge\n",
        "2. **SnowPro Advanced Certifications**: Specialized areas (Data Engineer, Data Scientist, etc.)\n",
        "3. **SnowPro Architect Certification**: Advanced architecture and design\n",
        "4. **Continuous Learning**: Stay updated with new features and best practices\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Troubleshooting & Common Issues\n",
        "\n",
        "## Common Implementation Issues\n",
        "\n",
        "### **Connection Issues**\n",
        "- **Problem**: Cannot connect to Snowflake\n",
        "- **Solution**: Verify account URL, username, password, and warehouse settings\n",
        "- **Check**: Network connectivity and firewall settings\n",
        "- **Verify**: Account status and user permissions\n",
        "\n",
        "### **Permission Errors**\n",
        "- **Problem**: \"Insufficient privileges\" errors\n",
        "- **Solution**: Grant appropriate roles and permissions\n",
        "- **Check**: Role hierarchy and privilege inheritance\n",
        "- **Verify**: Object ownership and access policies\n",
        "\n",
        "### **Performance Issues**\n",
        "- **Problem**: Slow query execution\n",
        "- **Solution**: Optimize queries, use appropriate warehouse size\n",
        "- **Check**: Query execution plan and resource usage\n",
        "- **Verify**: Data clustering and partitioning strategies\n",
        "\n",
        "### **Data Quality Issues**\n",
        "- **Problem**: DMFs not working as expected\n",
        "- **Solution**: Verify table permissions and DMF syntax\n",
        "- **Check**: Data types and column names\n",
        "- **Verify**: Schedule configuration and warehouse availability\n",
        "\n",
        "## Best Practices Checklist\n",
        "\n",
        "### **Security**\n",
        "- [ ] Use least privilege principle for roles\n",
        "- [ ] Implement data masking for sensitive data\n",
        "- [ ] Set up row-level security where needed\n",
        "- [ ] Regular access reviews and audits\n",
        "- [ ] Enable multi-factor authentication\n",
        "\n",
        "### **Performance**\n",
        "- [ ] Right-size warehouses for workloads\n",
        "- [ ] Use clustering keys for large tables\n",
        "- [ ] Implement materialized views for frequent queries\n",
        "- [ ] Monitor and optimize query performance\n",
        "- [ ] Set up auto-suspend for cost optimization\n",
        "\n",
        "### **Data Governance**\n",
        "- [ ] Classify sensitive data appropriately\n",
        "- [ ] Document data lineage and business terms\n",
        "- [ ] Implement data quality monitoring\n",
        "- [ ] Set up retention policies\n",
        "- [ ] Regular compliance reviews\n",
        "\n",
        "### **Cost Management**\n",
        "- [ ] Monitor credit usage and costs\n",
        "- [ ] Set up resource monitors and alerts\n",
        "- [ ] Optimize warehouse sizing\n",
        "- [ ] Review and clean up unused objects\n",
        "- [ ] Implement cost allocation strategies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Appendix: Quick Reference Guide\n",
        "\n",
        "## SQL Quick Reference\n",
        "\n",
        "### **Common DDL Commands**\n",
        "```sql\n",
        "-- Create database\n",
        "CREATE DATABASE database_name;\n",
        "\n",
        "-- Create schema\n",
        "CREATE SCHEMA schema_name;\n",
        "\n",
        "-- Create table\n",
        "CREATE TABLE table_name (column1 TYPE, column2 TYPE);\n",
        "\n",
        "-- Create view\n",
        "CREATE VIEW view_name AS SELECT * FROM table_name;\n",
        "\n",
        "-- Create materialized view\n",
        "CREATE MATERIALIZED VIEW mv_name AS SELECT * FROM table_name;\n",
        "```\n",
        "\n",
        "### **Common DML Commands**\n",
        "```sql\n",
        "-- Insert data\n",
        "INSERT INTO table_name VALUES (value1, value2);\n",
        "\n",
        "-- Update data\n",
        "UPDATE table_name SET column = value WHERE condition;\n",
        "\n",
        "-- Delete data\n",
        "DELETE FROM table_name WHERE condition;\n",
        "\n",
        "-- Select data\n",
        "SELECT column1, column2 FROM table_name WHERE condition;\n",
        "```\n",
        "\n",
        "### **Security Commands**\n",
        "```sql\n",
        "-- Create role\n",
        "CREATE ROLE role_name;\n",
        "\n",
        "-- Grant privileges\n",
        "GRANT SELECT ON TABLE table_name TO ROLE role_name;\n",
        "\n",
        "-- Create masking policy\n",
        "CREATE MASKING POLICY policy_name AS (val STRING) RETURNS STRING -> '***';\n",
        "\n",
        "-- Create row access policy\n",
        "CREATE ROW ACCESS POLICY policy_name AS (col STRING) RETURNS BOOLEAN -> TRUE;\n",
        "```\n",
        "\n",
        "## Horizon Catalog Features Summary\n",
        "\n",
        "| Feature | Description | Use Case |\n",
        "|---------|-------------|----------|\n",
        "| Iceberg Tables | Open table format with ACID transactions | Data lake integration, schema evolution |\n",
        "| DMFs | Data Metric Functions for quality monitoring | Automated data quality checks |\n",
        "| Business Vocabulary | Centralized business terminology | Data governance, business alignment |\n",
        "| Multi-Catalog | Support for multiple data sources | Data federation, unified querying |\n",
        "| Advanced Security | Row-level, column-level, aggregation policies | Compliance, data protection |\n",
        "| Data Lineage | Track data flow and transformations | Governance, impact analysis |\n",
        "\n",
        "## Contact & Support\n",
        "\n",
        "- **Snowflake Support**: [support.snowflake.com](https://support.snowflake.com)\n",
        "- **Documentation**: [docs.snowflake.com](https://docs.snowflake.com)\n",
        "- **Community Forum**: [community.snowflake.com](https://community.snowflake.com)\n",
        "- **Training**: [learn.snowflake.com](https://learn.snowflake.com)\n",
        "\n",
        "---\n",
        "\n",
        "**Notebook Version**: 1.0  \n",
        "**Last Updated**: January 2025  \n",
        "**Total Sections**: 19  \n",
        "**Total Code Cells**: 52  \n",
        "\n",
        "*This notebook provides a comprehensive guide to Snowflake Horizon Catalog implementation. For the latest updates and features, please refer to the official Snowflake documentation.*\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
